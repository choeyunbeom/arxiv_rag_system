{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning: Qwen3-4B for RAG\n",
    "\n",
    "- **Hardware**: Apple M4 Pro 48GB (MPS)\n",
    "- **Data**: 1,997 synthetic Q&A pairs (grounded / synthesis / refusal)\n",
    "- **Method**: LoRA on bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/choeyunbeom/Desktop/new_project/arxiv_rag_system/src/finetuning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/choeyunbeom/Desktop/new_project/arxiv_rag_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Config ──\n",
    "BASE_MODEL_DIR = Path(\"data/base_model\")\n",
    "DATASET_PATH = Path(\"data/processed/qa_dataset.json\")\n",
    "OUTPUT_DIR = Path(\"data/finetuned_lora\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 2               # Try 4 if memory allows\n",
    "GRAD_ACCUM_STEPS = 8         # Effective batch = 2 * 8 = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "WARMUP_RATIO = 0.05\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1997\n",
      "Types: {'refusal': 400, 'grounded': 1200, 'synthesis': 397}\n",
      "Keys: ['type', 'instruction', 'input', 'output', 'source_arxiv_id']\n"
     ]
    }
   ],
   "source": [
    "with open(DATASET_PATH) as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "samples = raw[\"data\"]\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "print(f\"Types: {dict(Counter(s['type'] for s in samples))}\")\n",
    "print(f\"Keys: {list(samples[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GROUNDED SAMPLE ===\n",
      "instruction: You are a helpful academic research assistant. Answer questions based ONLY on the provided context from academic papers. Follow these rules strictly:\n",
      "...\n",
      "input: Context from 'Differentially Private Fine-tuning of Language Models' (abstract):\n",
      "##metrizedgradientperturbation _ ( rgp ). rgp exploits the implicit low - rank structure in the gradient updates of sgd...\n",
      "output: The paper 'Differentially Private Fine-tuning of Language Models' explains that metrized gradient perturbation (RGP) exploits the implicit low-rank structure in SGD gradient updates to substantially i...\n"
     ]
    }
   ],
   "source": [
    "# Preview one sample\n",
    "s = next(s for s in samples if s[\"type\"] == \"grounded\")\n",
    "print(\"=== GROUNDED SAMPLE ===\")\n",
    "print(f\"instruction: {s['instruction'][:150]}...\")\n",
    "print(f\"input: {s['input'][:200]}...\")\n",
    "print(f\"output: {s['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 151643\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(BASE_MODEL_DIR),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9610f51445994fed92005ac60d67a21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(BASE_MODEL_DIR),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format Data → Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted: 1997, Skipped: 0\n"
     ]
    }
   ],
   "source": [
    "def format_to_chat(sample: dict) -> str:\n",
    "    \"\"\"instruction → system, input → user, output → assistant\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sample[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": sample[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": sample[\"output\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "\n",
    "formatted = []\n",
    "skipped = 0\n",
    "for i, sample in enumerate(samples):\n",
    "    try:\n",
    "        text = format_to_chat(sample)\n",
    "        formatted.append({\"text\": text})\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped sample {i}: {e}\")\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"Formatted: {len(formatted)}, Skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful academic research assistant. Answer questions based ONLY on the provided context from academic papers. Follow these rules strictly:\n",
      "1. Only use information from the provided context\n",
      "2. Cite which paper the information comes from\n",
      "3. If the context does not contain enough information, say so clearly\n",
      "4. Answer in concise prose paragraphs without markdown headers or bullet points\n",
      "5. Do not generalise findings from one paper as universal recommendations<|im_end|>\n",
      "<|im_start|>user\n",
      "Context from 'FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation' (conclusion):\n",
      "our current work open up several promising directions for future research to create more efficient and adaptive systems : - * * distilling task - specific expert models :\n"
     ]
    }
   ],
   "source": [
    "# Preview formatted output\n",
    "print(formatted[0][\"text\"][:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token lengths — min: 257, max: 841, mean: 377\n",
      "Over 2048 tokens: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check token lengths\n",
    "lengths = [len(tokenizer.encode(f[\"text\"])) for f in formatted]\n",
    "print(f\"Token lengths — min: {min(lengths)}, max: {max(lengths)}, mean: {sum(lengths)/len(lengths):.0f}\")\n",
    "over_limit = sum(1 for l in lengths if l > MAX_SEQ_LENGTH)\n",
    "print(f\"Over {MAX_SEQ_LENGTH} tokens: {over_limit} ({over_limit/len(lengths)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1897, Eval: 100\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(formatted)\n",
    "split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"Train: {len(split['train'])}, Eval: {len(split['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7914123883fe400e864581af5c8c6ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f60b1f5c9e46508d1a44e90693e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939f13374044deeb09ca9ecb1ff901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d320fc689406ba73feadff1c1c2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae98773df4a417492ba2a4655f20287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9837377ba54096b8f2e11bf44e2d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split[\"train\"],\n",
    "    eval_dataset=split[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 6:49:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.105551</td>\n",
       "      <td>1.117962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.022651</td>\n",
       "      <td>1.060225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.881753</td>\n",
       "      <td>1.064029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 24626s (410.4 min)\n",
      "Final train loss: 1.1279\n",
      "Samples/sec: 0.231\n"
     ]
    }
   ],
   "source": [
    "# Training summary\n",
    "runtime = result.metrics[\"train_runtime\"]\n",
    "print(f\"Training time: {runtime:.0f}s ({runtime/60:.1f} min)\")\n",
    "print(f\"Final train loss: {result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Samples/sec: {result.metrics.get('train_samples_per_second', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to data/finetuned_lora/final\n",
      "Metrics saved\n"
     ]
    }
   ],
   "source": [
    "final_dir = OUTPUT_DIR / \"final\"\n",
    "trainer.save_model(str(final_dir))\n",
    "tokenizer.save_pretrained(str(final_dir))\n",
    "print(f\"Adapter saved to {final_dir}\")\n",
    "\n",
    "# Save metrics\n",
    "with open(OUTPUT_DIR / \"training_metrics.json\", \"w\") as f:\n",
    "    json.dump(result.metrics, f, indent=2)\n",
    "print(\"Metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Sanity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "QLoRA is a method that reduces memory usage enough to fine-tune a 65B parameter model on a single 48GB GPU while preserving full 16-bit fine-tuning task performance.\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a sample question\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful academic research assistant. Answer questions based ONLY on the provided context from academic papers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Context from 'QLoRA: Efficient Finetuning of Quantized Language Models' (abstract):\\nQLoRA reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\\n\\nQuestion: What is QLoRA?\"},\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.3,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert to GGUF for Ollama (Optional)\n",
    "\n",
    "To serve via Ollama, merge LoRA + convert to GGUF:\n",
    "\n",
    "```bash\n",
    "# Merge base + LoRA\n",
    "python -c \"\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "base = AutoModelForCausalLM.from_pretrained('data/base_model')\n",
    "model = PeftModel.from_pretrained(base, 'data/finetuned_lora/final')\n",
    "merged = model.merge_and_unload()\n",
    "merged.save_pretrained('data/merged_model')\n",
    "\"\n",
    "\n",
    "# Convert to GGUF (requires llama.cpp)\n",
    "python llama.cpp/convert_hf_to_gguf.py data/merged_model \\\n",
    "    --outfile data/qwen3-4b-rag.gguf --outtype q4_K_M\n",
    "\n",
    "# Register with Ollama\n",
    "echo 'FROM data/qwen3-4b-rag.gguf' > Modelfile\n",
    "ollama create qwen3-4b-rag -f Modelfile\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
