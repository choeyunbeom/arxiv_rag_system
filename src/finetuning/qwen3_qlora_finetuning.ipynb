{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning: Qwen3 4B for Academic RAG\n",
    "\n",
    "Fine-tune Qwen3 4B on 1997 synthetic Q&A pairs generated from 132 arXiv papers.\n",
    "\n",
    "**Training Data Types:**\n",
    "- Type 1 (60%): Context-grounded answering with source attribution\n",
    "- Type 2 (20%): Multi-paper synthesis\n",
    "- Type 3 (20%): Refusal when context is insufficient\n",
    "\n",
    "**Runtime:** T4 GPU (Colab free tier)\n",
    "**Framework:** Unsloth + TRL SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Unsloth optimised — 0 is faster\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload qa_dataset.json from local machine\n",
    "print(\"Upload qa_dataset.json:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "with open(\"qa_dataset.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {raw_data['total']} Q&A pairs\")\n",
    "print(f\"Stats: {raw_data['stats']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Format into chat template\n",
    "def format_chat(example):\n",
    "    \"\"\"Convert to Qwen3 chat format with /no_think to disable thinking mode.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False,\n",
    "        enable_thinking=False,  # Disable thinking for training\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = Dataset.from_list(raw_data[\"data\"])\n",
    "dataset = dataset.map(format_chat)\n",
    "\n",
    "# Train/val split\n",
    "split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "print(f\"\\nSample (first 500 chars):\\n{train_dataset[0]['text'][:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        # Batch size\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "        \n",
    "        # Learning rate\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        \n",
    "        # Duration\n",
    "        num_train_epochs=3,\n",
    "        \n",
    "        # Precision\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        \n",
    "        # Output\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {trainer.state.max_steps if hasattr(trainer.state, 'max_steps') else 'TBD'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_mem / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Memory: {start_gpu_memory}GB / {max_memory}GB\")\n",
    "\n",
    "# Train!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Report\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Duration: {trainer_stats.metrics['train_runtime']:.0f}s ({trainer_stats.metrics['train_runtime']/60:.1f} min)\")\n",
    "print(f\"  Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Peak GPU memory: {used_memory}GB / {max_memory}GB ({100*used_memory/max_memory:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is QLoRA and how does it reduce memory usage?\",\n",
    "    \"Compare different approaches to reducing hallucination in RAG systems.\",\n",
    "    \"What is the capital of France?\",  # Should refuse — not in context\n",
    "]\n",
    "\n",
    "test_context = \"\"\"Context from 'QLoRA: Efficient Finetuning of Quantized LLMs' (methodology):\n",
    "QLoRA backpropagates gradients through a frozen 4-bit quantized pretrained language model into Low Rank Adapters. It introduces NF4 quantisation and Double Quantisation to reduce memory footprint.\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful academic research assistant. Answer questions based ONLY on the provided context from academic papers. Follow these rules strictly:\n",
    "1. Only use information from the provided context\n",
    "2. Cite which paper the information comes from\n",
    "3. If the context does not contain enough information, say so clearly\n",
    "4. Answer in concise prose paragraphs without markdown headers or bullet points\n",
    "5. Do not generalise findings from one paper as universal recommendations\"\"\"\n",
    "\n",
    "for q in test_questions:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{test_context}\\n\\nQuestion: {q}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True,\n",
    "        enable_thinking=False, return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=256,\n",
    "        temperature=0.3, top_p=0.9,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {response[:300]}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"qwen3-4b-arxiv-rag-lora\")\n",
    "tokenizer.save_pretrained(\"qwen3-4b-arxiv-rag-lora\")\n",
    "print(\"LoRA adapters saved locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your HF token\n",
    "\n",
    "model.push_to_hub(\"choeyunbeom/qwen3-4b-arxiv-rag\", tokenizer=tokenizer)\n",
    "print(\"Pushed to HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF for Ollama\n",
    "model.save_pretrained_gguf(\n",
    "    \"qwen3-4b-arxiv-rag-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(\"GGUF exported! Download and use with Ollama:\")\n",
    "print(\"  ollama create qwen3-arxiv -f Modelfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GGUF file\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "gguf_files = glob.glob(\"qwen3-4b-arxiv-rag-gguf/*.gguf\")\n",
    "for f in gguf_files:\n",
    "    print(f\"Downloading {f}...\")\n",
    "    files.download(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary\n",
    "\n",
    "Record the key metrics for comparison with baseline:\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Base model | Qwen3 4B (4-bit) |\n",
    "| LoRA rank | 16 |\n",
    "| Target modules | q,k,v,o_proj + gate,up,down_proj |\n",
    "| Training data | 1997 Q&A pairs |\n",
    "| Epochs | 3 |\n",
    "| Effective batch size | 16 |\n",
    "| Learning rate | 2e-4 (cosine) |\n",
    "| Final train loss | (fill after training) |\n",
    "| Training time | (fill after training) |\n",
    "| Peak GPU memory | (fill after training) |"
   ]
  }
 ]
}
