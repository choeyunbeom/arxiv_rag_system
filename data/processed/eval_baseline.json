{
  "label": "baseline",
  "timestamp": "2026-02-27T17:23:57.062902",
  "config": {
    "top_k": 5,
    "dataset_size": 15
  },
  "retrieval": {
    "hit_rate": 1.0,
    "mrr": 0.8166666666666667,
    "avg_precision": 0.4400000000000001,
    "total_questions": 15,
    "top_k": 5
  },
  "answer": {
    "summary": {
      "avg_keyword_coverage": 0.78,
      "source_hit_rate": 1.0,
      "substantive_rate": 1.0,
      "avg_word_count": 172.06666666666666,
      "avg_latency": 20.397695604960123,
      "total_time": 305.97,
      "total_questions": 15
    },
    "details": [
      {
        "question": "What is QLoRA and how does it reduce memory usage?",
        "topic": "qlora",
        "answer": "QLoRA (Quantized Low-Rank Optimization for Fine-tuning) is an efficient fine-tuning approach that reduces memory usage enough to fine-tune large language models (up to 65B parameters) on a single 48GB GPU while preserving full 16-bit fine-tuning performance. \n\nIt reduces memory usage through three key innovations:\n1. **4-bit NormalFloat (NF4)** - An information-theoretically optimal quantization data type for normally distributed weights that outperforms 4-bit integers and floats\n2. **Double Quantization** - Quantizes the quantization constants, saving approximately 0.37 bits per parameter (about 3GB for a 65B model)\n3. **Paged Optimizers** - Uses NVIDIA unified memory to avoid gradient checkpointing memory spikes during training\n\nQLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA), converting 16-bit or 32-bit floating-point weights to 4-bit integers. This approach reduces memory footprint by approximately 60% while maintaining model accuracy through the hybrid approach of frozen quantized weights and precise updates from full-precision low-rank matrices.\n\nThe primary source for this information is \"QLoRA: Efficient Finetuning of Quantized LLMs\" (papers [1] and [4]).",
        "answer_word_count": 170,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2305.14314v1",
          "2506.03178v1",
          "2401.00503v1",
          "2505.03406v1"
        ],
        "expected_ids": [
          "2305.14314v1",
          "2505.03406v1"
        ],
        "is_substantive": true,
        "latency_seconds": 22.29
      },
      {
        "question": "How does LoRA work for fine-tuning large language models?",
        "topic": "lora",
        "answer": "# How LoRA Works for Fine-tuning Large Language Models\n\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique that works by freezing the pretrained model weights and inserting low-rank matrices into the model architecture, significantly reducing computational and memory requirements while maintaining performance [2]. \n\nSpecifically, LoRA allows for efficient adaptation of large language models by adjusting only a small set of parameters, which dramatically reduces computational and storage costs compared to full fine-tuning [1]. This approach achieves over 99% reduction in trainable parameter count depending on the configuration, making it particularly suitable for resource-constrained environments [5].\n\nLoRA is modular, meaning it can be added or removed from a pretrained model without altering the original weights, and has a low memory footprint requiring very little additional storage for the trainable parameters [4]. The technique assumes that the optimization process is essentially low-dimensional, which enables efficient adaptation while preserving model performance [3].\n\nBy focusing on low-rank updates to specific model components (such as attention modules or feed-forward layers), LoRA enables targeted fine-tuning that maintains high performance while minimizing the computational resources required [2].",
        "answer_word_count": 181,
        "keyword_coverage": 0.4,
        "keywords_found": 2,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2402.01717v1",
          "2506.20629v1",
          "2412.09827v1",
          "2512.17983v1",
          "2602.05988v1"
        ],
        "expected_ids": [
          "2106.09685v2",
          "2512.17983v1",
          "2602.05988v1"
        ],
        "is_substantive": true,
        "latency_seconds": 19.41
      },
      {
        "question": "What is Retrieval Augmented Generation?",
        "topic": "rag",
        "answer": "Retrieval Augmented Generation (RAG) is a paradigm that enhances large language models (LLMs) by combining their generative abilities with access to external knowledge during inference time. According to [2], a typical RAG framework comprises two core components: a retriever that selects relevant documents from a large corpus, and a generator that synthesizes information from these documents to produce coherent and contextually appropriate responses. [5] describes RAG as enhancing LLM capabilities by retrieving relevant passages from an external database during inference time to ground responses in factual context, rather than relying solely on parametric memory.",
        "answer_word_count": 94,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2402.12317v2",
          "2502.13957v2",
          "2402.01717v1",
          "2507.23334v2"
        ],
        "expected_ids": [
          "2601.05264v1",
          "2502.13957v2",
          "2401.15391v1",
          "2502.00306v2"
        ],
        "is_substantive": true,
        "latency_seconds": 26.54
      },
      {
        "question": "How can RAG systems be evaluated?",
        "topic": "rag_eval",
        "answer": "Based on the provided context, RAG systems can be evaluated through several approaches:\n\n1. **Automated reference-free evaluation frameworks** - The Ragas framework (paper [1], [2]) provides automated assessment of RAG systems without requiring reference answers. It specifically evaluates three key aspects: faithfulness (is the answer grounded in retrieved context), answer relevance (does the answer address the question), and context relevance (is the retrieved context sufficiently focused).\n\n2. **Benchmarking datasets** - Multiple datasets exist for RAG evaluation including:\n   - HotpotQA (multi-hop questions on Wikipedia with 113k QA pairs)\n   - FEVER (factual claims with verification)\n   - RGB (multi-capability evaluation)\n   - Marco (search queries on Wikipedia)\n   - These datasets vary in complexity, domain, and evaluation focus (paper [5])\n\n3. **User-centric evaluation** - Competitions like MMU-RAG (paper [4]) implement user-centric evaluation that combines:\n   - Static and dynamic evaluation modes\n   - Robustness-aware aggregation of normalized metrics\n   - Human judgments based on ordinal Likert-scale ratings\n\n4. **Enterprise evaluation platforms** - Systems like Galileo and LangSmith provide infrastructure for real-time monitoring, automated evaluation pipelines, and integration with development workflows (paper [5])\n\nThe context shows that evaluation approaches vary from automated metrics to human judgment, with specific focus on whether reference answers are available and what aspects of RAG performance need assessment (faithfulness, relevance, etc.).",
        "answer_word_count": 207,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.15217v2",
          "2401.15391v1",
          "2602.20735v1",
          "2601.05264v1"
        ],
        "expected_ids": [
          "2309.15217v2",
          "2502.00306v2",
          "2601.05264v1"
        ],
        "is_substantive": true,
        "latency_seconds": 20.92
      },
      {
        "question": "What techniques exist to mitigate hallucinations in LLMs?",
        "topic": "hallucination",
        "answer": "# Techniques to Mitigate Hallucinations in LLMs\n\nBased on the provided context, several techniques exist to mitigate hallucinations in LLMs:\n\n1. **Retrieval-Augmented Generation (RAG) methods**: Lightweight RAG-based approaches can effectively mitigate hallucinations in code generation (Paper [1]). Rowen (Paper [4]) implements an adaptive RAG system that integrates parametric knowledge with retrieved sources, achieving 59.34% GPT-Judge score on TruthfulQA.\n\n2. **Self-monitoring prompting frameworks**: Jha et al. proposed a framework that uses formal methods to identify errors in LLM responses autonomously through iterative refinement (Paper [2]).\n\n3. **Self-familiarity**: Luo et al. developed a zero-resource, pre-detection approach that extracts conceptual entities from instructions and uses prompt engineering to acquire familiarity scores, with low scores indicating higher likelihood of erroneous information (Paper [2]).\n\n4. **Consortium consistency**: Combining responses from multiple LLMs with different training data and architectures shows substantial improvements in hallucination detection and mitigation beyond single-model approaches (Paper [3]).\n\n5. **Context-tagged prompts**: Feldman et al. created context prompts to help LLMs answer questions more accurately through validated question sets (Paper [2]).\n\n6. **Multi-agent systems**: Iterative processes that clarify task requirements, generate code, run test cases, and mitigate hallucinations (Paper [1]).\n\n7. **Adaptive retrieval strategies**: Techniques that assess question difficulty or response confidence to decide whether to retrieve documents (Paper [5]).\n\nThese techniques address different aspects of hallucination mitigation, from pre-detection to post-hallucination correction, with varying degrees of effectiveness depending on the specific use case and context.",
        "answer_word_count": 233,
        "keyword_coverage": 0.4,
        "keywords_found": 2,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2409.20550v2",
          "2311.08117v1",
          "2510.19507v2",
          "2402.10612v3"
        ],
        "expected_ids": [
          "2311.08117v1",
          "2409.20550v2",
          "2507.15903v1",
          "2510.19507v2",
          "2502.11306v1"
        ],
        "is_substantive": true,
        "latency_seconds": 22.07
      },
      {
        "question": "What is instruction tuning and why is it important?",
        "topic": "instruction_tuning",
        "answer": "Instruction tuning is a fine-tuning technique for large language models (LLMs) that involves training them on specific instruction-response pairs to improve their ability to follow user requests. According to Paper [1], it significantly enhances LLM performance across various tasks. Paper [3] states that instruction tuning was initially designed to enhance the generalization capabilities of LMs across different tasks via multitask training. Its importance includes enabling zero-shot instruction following (Paper [4]), improving domain-specific capabilities (Paper [5]), and facilitating better performance in low-resource languages.",
        "answer_word_count": 82,
        "keyword_coverage": 0.8,
        "keywords_found": 4,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2312.10793v3",
          "2409.14254v1",
          "2306.08568v2",
          "2405.19744v1",
          "2310.04793v2"
        ],
        "expected_ids": [
          "2304.07995v1",
          "2409.14254v1",
          "2304.03277v1",
          "2304.12244v3"
        ],
        "is_substantive": true,
        "latency_seconds": 18.89
      },
      {
        "question": "How does LoRA+ improve upon standard LoRA?",
        "topic": "lora_plus",
        "answer": "Based on the provided context, LoRA+ improves upon standard LoRA by applying different learning rates to the two low-rank adapter matrices (which in standard LoRA are updated with the same learning rate). \n\nAs stated in Paper [2] (\"LoRA+: Efficient Low Rank Adaptation of Large Models\"), this approach addresses a fundamental limitation in standard LoRA: \"low rank adaptation (lora) as originally introduced in (hu et al., 2021) leads to suboptimal finetuning of models with large width (embedding dimension). this is due to the fact that adapter matrices and in lora are updated with the same learning rate.\" \n\nThe paper demonstrates that using different learning rates for the adapter matrices with a well-chosen fixed ratio corrects this suboptimality. The results show that LoRA+ improves performance by 1-2% and finetuning speed by up to 2x, while maintaining the same computational cost as standard LoRA.\n\nThis improvement is specifically designed for large models with high embedding dimensions where standard LoRA performs suboptimally due to the uniform learning rate application to both low-rank matrices.",
        "answer_word_count": 169,
        "keyword_coverage": 0.4,
        "keywords_found": 2,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2407.18242v3",
          "2402.12354v2",
          "2506.20629v1",
          "2602.05988v1",
          "2106.09685v2"
        ],
        "expected_ids": [
          "2402.12354v2"
        ],
        "is_substantive": true,
        "latency_seconds": 18.1
      },
      {
        "question": "What is the difference between full fine-tuning and parameter-efficient fine-tuning?",
        "topic": "peft",
        "answer": "Based on the provided context, here's the difference between full fine-tuning and parameter-efficient fine-tuning:\n\nFull fine-tuning updates all parameters of a pretrained neural network when adapting it to a new task or domain (Paper [2]). It maximizes domain-specific learning capacity but at the expense of higher compute requirements, sensitivity to initialization, and risk of forgetting pretrained knowledge. Full fine-tuning is particularly effective when the target task differs significantly from pretraining, often yielding higher accuracy in transfer scenarios with small datasets or strong domain shifts. However, it is rarely suited for edge deployment due to memory and compute intensity.\n\nParameter-efficient fine-tuning (PEFT) introduces a limited set of additional trainable parameters while keeping most pre-trained parameters frozen (Paper [4]). Techniques like LoRA reduce the number of trainable parameters from m√ón to (k + 1) where k is the rank, thereby decreasing fine-tuning cost (Paper [3]). PEFT methods such as LoRA use low-rank matrices to approximate weight updates, making them more memory and compute efficient. This efficiency allows for practical deployment in edge scenarios and large-scale personalization contexts.\n\nThe key difference is that full fine-tuning updates all model parameters while PEFT techniques maintain most pre-trained parameters frozen and only update a small number of additional parameters (typically low-rank matrices), significantly reducing computational and memory requirements while still achieving comparable performance to full fine-tuning in many cases (Paper [1] notes that full fine-tuning via DPSGD achieves same performance as parameter-efficient methods).",
        "answer_word_count": 237,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2110.06500v2",
          "2512.17983v1",
          "2407.18242v3",
          "2411.14961v3",
          "2602.05988v1"
        ],
        "expected_ids": [
          "2106.09685v2",
          "2305.14314v1",
          "2512.17983v1",
          "2412.09827v1"
        ],
        "is_substantive": true,
        "latency_seconds": 23.03
      },
      {
        "question": "How do multi-hop questions challenge RAG systems?",
        "topic": "multihop_rag",
        "answer": "Based on the provided context, multi-hop questions challenge RAG systems in several key ways:\n\n1. Retrieval challenges: The context indicates that \"the low values of the retrieval metrics highlight the challenges in retrieving relevant pieces of evidence for multi-hop queries when using direct similarity matching between the multihop query and text chunks\" (Paper: MultiHop-RAG).\n\n2. Reasoning limitations: Existing RAG systems are \"inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence\" (Paper: MultiHop-RAG).\n\n3. Information noise issues: When RAG systems use retrieved information directly as input, \"the retrieved information incorporates some noises and makes the llm lost in the useful information\" (Paper: RAG-Star).\n\n4. Model capability constraints: The context shows that even state-of-the-art models struggle with multi-hop queries, with smaller models achieving \"below 10% across three metrics in musique\" (Paper: RAG-Star).\n\nThese challenges stem from the need for RAG systems to handle multiple retrieval steps and reason across different evidence pieces, which current implementations struggle with due to both technical limitations in retrieval and the complexity of multi-step reasoning.\n\nThe most comprehensive answer comes from the MultiHop-RAG paper which states that \"existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence\" (Paper: MultiHop-RAG).",
        "answer_word_count": 209,
        "keyword_coverage": 0.8,
        "keywords_found": 4,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2401.15391v1",
          "2502.13957v2",
          "2412.12881v1"
        ],
        "expected_ids": [
          "2401.15391v1",
          "2502.13957v2"
        ],
        "is_substantive": true,
        "latency_seconds": 21.95
      },
      {
        "question": "What is the role of vector databases in RAG pipelines?",
        "topic": "vector_db",
        "answer": "Vector databases in RAG pipelines serve as the semantic retrieval component that stores document embeddings to enable efficient context retrieval. Based on the context:\n\nThey convert text documents into dense vector representations (using domain-specific embedding models like e5-large-v2) that capture semantic meaning more effectively than general-purpose embeddings [5]. These embedding vectors are stored in vector databases (such as Pinecone) which support fast and accurate similarity searches through metrics like cosine similarity [5].\n\nThe primary role is to provide fine-grained retrieval of relevant document chunks (typically around 512 tokens) that act as context for the LLM generation component [5]. Vector databases help filter irrelevant information while preserving context length, which is critical since modern AI models have limited context capacity [4].\n\nThey also enable periodic updates with new or revised data sources (e.g., clinical guidelines) to maintain system accuracy with evolving information [5]. This continuous updating process ensures the RAG system stays current with the most relevant knowledge.\n\nVector databases function as the bridge between user queries and textual databases, allowing LLMs to access relevant information without hallucinations by providing focused context passages [2].",
        "answer_word_count": 183,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2505.12524v1",
          "2309.15217v2",
          "2601.05264v1",
          "2306.15295v2",
          "2505.03406v1"
        ],
        "expected_ids": [
          "2601.05264v1",
          "2505.12524v1",
          "2502.00306v2"
        ],
        "is_substantive": true,
        "latency_seconds": 19.12
      },
      {
        "question": "How can prompt engineering improve LLM outputs?",
        "topic": "prompt_engineering",
        "answer": "Based on the provided context, prompt engineering improves LLM outputs through several specific techniques:\n\n1. **Zero-shot prompting** - Providing only the task description to get desired outputs (Paper [1])\n\n2. **Few-shot prompting** - Giving a few examples of desired output as part of the prompt, which has been observed to improve output quality compared to alternative methods (Paper [2])\n\n3. **Chain-of-thought reasoning** - Asking the LLM to explain its reasoning and work through answering step by step, which consistently produces improved outputs and helps prevent undesirable behaviors like hallucination (Paper [1])\n\n4. **Improved context and structure** - The \"context\" and \"structure\" of prompts significantly impact the quality of generated outputs (Paper [2])\n\n5. **Reducing reasoning mistakes** - Prompt engineering has been shown to reduce reasoning mistakes and improve performance in various areas (Paper [3])\n\nThese techniques work by guiding the LLM toward desired outcomes without modifying the underlying model parameters, providing a lightweight and scalable method for influencing LLM behavior while helping to prevent issues like hallucinations and improving output quality.",
        "answer_word_count": 171,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.13734v2",
          "2507.03405v1",
          "2502.06039v1",
          "2503.02400v2"
        ],
        "expected_ids": [
          "2507.03405v1",
          "2309.13734v2",
          "2503.02400v2",
          "2401.14043v3"
        ],
        "is_substantive": true,
        "latency_seconds": 18.75
      },
      {
        "question": "What is Double Quantization in QLoRA?",
        "topic": "double_quant",
        "answer": "Double Quantization in QLoRA is a technique that quantizes the quantization constants themselves (not just the model weights), which saves memory while maintaining performance. According to the paper \"QLoRA: Efficient Finetuning of Quantized LLMs,\" this method saves an average of about 0.37 bits per parameter (approximately 3 GB for a 65b model). \n\nThe technical implementation involves two levels of quantization constants (c1 and c2), where c1 represents the second level and c2 represents the first level of quantization constants. This is detailed in the paper \"Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding\" which shows how the doubledequant() function combines these levels to maintain the model's functionality.\n\nThis technique is one of the three key innovations in QLoRA that helps reduce memory usage without sacrificing performance, alongside 4-bit normalfloat quantization and paged optimizers.",
        "answer_word_count": 135,
        "keyword_coverage": 0.6,
        "keywords_found": 3,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2305.14314v1",
          "2401.00503v1",
          "2404.00862v1",
          "2505.03406v1",
          "2506.03178v1"
        ],
        "expected_ids": [
          "2305.14314v1",
          "2404.00862v1"
        ],
        "is_substantive": true,
        "latency_seconds": 20.27
      },
      {
        "question": "How does the Ragas framework evaluate RAG pipelines?",
        "topic": "ragas",
        "answer": "Based on the provided context, the Ragas framework evaluates RAG pipelines through a suite of reference-free metrics that assess key dimensions of RAG system quality without requiring ground truth human annotations.\n\nSpecifically, Ragas employs LLM-based judges to evaluate response quality through four core metrics:\n1. Faithfulness (whether the answer is grounded in the retrieved context)\n2. Answer relevancy (whether the answer addresses the question)\n3. Context precision (how focused the retrieved context is)\n4. Context recall (how much relevant context is retrieved)\n\nAs stated in Paper [1], Ragas provides a framework that evaluates \"the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself\" without ground truth annotations.\n\nPaper [4] confirms that Ragas implementation aligns with human predictions, particularly for faithfulness and answer relevance, and specifically evaluates three critical aspects: faithfulness, answer relevance, and context relevance.\n\nThe framework is designed to be easy to use and provides valuable insights for RAG system developers even in the absence of ground truth data.\n\nCitations: [1] Ragas: Automated Evaluation of Retrieval Augmented Generation; [4] Ragas: Automated Evaluation of Retrieval Augmented Generation.",
        "answer_word_count": 200,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.15217v2",
          "2601.05264v1",
          "2502.13957v2",
          "2510.22344v1"
        ],
        "expected_ids": [
          "2309.15217v2",
          "2601.05264v1"
        ],
        "is_substantive": true,
        "latency_seconds": 17.76
      },
      {
        "question": "What are corpus poisoning attacks on RAG systems?",
        "topic": "rag_security",
        "answer": "Based on the provided context, corpus poisoning attacks on RAG systems refer to attacks where adversaries inject malicious documents into the RAG knowledge base to manipulate system behavior. \n\nSpecifically, in e-commerce contexts, a malicious seller could add poisoned documents to make their products retrievable for irrelevant queries to increase sales. In legal contexts, adversaries could try to make documents with incorrect judgments retrievable with malicious intentions. \n\nThese attacks are categorized as \"malicious poisoning document injection\" in the literature, which is identified as a high-risk attack type with high complexity and high rate (Paper [3]). Paper [1] specifically addresses these attacks by proposing RAGPart and RAGMask as retrieval-stage defenses against corpus poisoning in RAG systems.\n\nThe context shows that corpus poisoning attacks can be countered through technical defense mechanisms that protect against document injection into the RAG knowledge database.",
        "answer_word_count": 138,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2512.24268v1",
          "2502.00306v2",
          "2601.05264v1"
        ],
        "expected_ids": [
          "2512.24268v1"
        ],
        "is_substantive": true,
        "latency_seconds": 17.34
      },
      {
        "question": "How can small language models be made more efficient through fine-tuning?",
        "topic": "small_llm",
        "answer": "Based on the provided context, small language models can be made more efficient through fine-tuning using the following approaches:\n\n1. **Contrastive fine-tuning**: This technique enhances text embeddings for smaller language models (such as MiniCPM, Phi-2, and Gemma) by training them to distinguish between similar and dissimilar text pairs. This improves semantic understanding while maintaining computational efficiency (Paper [1] and [3]).\n\n2. **Parameter-efficient fine-tuning with LoRA (Low-Rank Adaptation)**: This method allows for efficient adaptation of small language models by adjusting only a small set of parameters, significantly reducing computational and storage costs. The approach was implemented using Hugging Face's PEFT library (Paper [2] and [3]).\n\n3. **Layer-wise LoRA fine-tuning**: This advanced technique identifies and fine-tunes only the most relevant layers of the model based on their contribution to changes in internal representations. This reduces trainable parameters by up to 50% while maintaining predictive performance across different models and tasks (Paper [4]).\n\nThese approaches specifically address the resource constraints of smaller language models while improving their performance on text embedding tasks and downstream applications.",
        "answer_word_count": 172,
        "keyword_coverage": 0.8,
        "keywords_found": 4,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2408.00690v2",
          "2402.01717v1",
          "2602.05988v1",
          "2408.07888v2"
        ],
        "expected_ids": [
          "2408.00690v2",
          "2504.16584v1",
          "2305.14314v1"
        ],
        "is_substantive": true,
        "latency_seconds": 19.52
      }
    ]
  }
}