{
  "label": "baseline",
  "timestamp": "2026-02-27T17:07:03.188040",
  "config": {
    "top_k": 5,
    "dataset_size": 15
  },
  "retrieval": {
    "hit_rate": 1.0,
    "mrr": 0.8166666666666667,
    "avg_precision": 0.4400000000000001,
    "total_questions": 15,
    "top_k": 5
  },
  "answer": {
    "summary": {
      "avg_keyword_coverage": 0.7400000000000001,
      "source_hit_rate": 1.0,
      "substantive_rate": 1.0,
      "avg_word_count": 174.4,
      "avg_latency": 21.360120995839438,
      "total_time": 320.4,
      "total_questions": 15
    },
    "details": [
      {
        "question": "What is QLoRA and how does it reduce memory usage?",
        "topic": "qlora",
        "answer": "QLoRA (Quantized Low-Rank Optimization for Adaptive Fine-Tuning) is an efficient finetuning approach for large language models that reduces memory usage while preserving performance. According to the paper \"QLoRA: Efficient Finetuning of Quantized LLMs\" [1], QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n\nQLoRA reduces memory usage through three key innovations:\n1. **4-bit NormalFloat (NF4)** - An information theoretically optimal quantization data type for normally distributed weights that outperforms 4-bit integers and floats\n2. **Double Quantization** - Quantizing the quantization constants to reduce the average memory footprint by approximately 0.37 bits per parameter (about 3 GB for a 65B model)\n3. **Paged Optimizers** - Using NVIDIA unified memory to avoid gradient checkpointing memory spikes during training\n\nThis approach converts original 16-bit or 32-bit floating-point weights to 4-bit integers, reducing memory footprint by approximately 60% while maintaining model performance. As demonstrated in the paper, this enables fine-tuning of large models (up to 65B parameters) on hardware with limited memory (single 48GB GPU), making it practical for resource-constrained environments.",
        "answer_word_count": 174,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2305.14314v1",
          "2506.03178v1",
          "2401.00503v1",
          "2505.03406v1"
        ],
        "expected_ids": [
          "2305.14314v1",
          "2505.03406v1"
        ],
        "is_substantive": true,
        "latency_seconds": 20.79
      },
      {
        "question": "How does LoRA work for fine-tuning large language models?",
        "topic": "lora",
        "answer": "LoRA (Low-Rank Adaptation) works by freezing the pretrained model weights and inserting low-rank matrices into the model architecture, significantly reducing computational and memory requirements for fine-tuning [2]. This parameter-efficient approach achieves over 99% reduction in trainable parameters compared to full fine-tuning [5]. LoRA is modular, meaning it can be added or removed from a pretrained model without altering the original weights, and has a low memory footprint requiring very little additional storage for the trainable parameters [4]. The technique enables efficient adaptation of large language models on limited hardware resources while maintaining performance, making it particularly valuable for production-scale applications [2].",
        "answer_word_count": 101,
        "keyword_coverage": 0.4,
        "keywords_found": 2,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2402.01717v1",
          "2506.20629v1",
          "2412.09827v1",
          "2512.17983v1",
          "2602.05988v1"
        ],
        "expected_ids": [
          "2106.09685v2",
          "2512.17983v1",
          "2602.05988v1"
        ],
        "is_substantive": true,
        "latency_seconds": 27.07
      },
      {
        "question": "What is Retrieval Augmented Generation?",
        "topic": "rag",
        "answer": "Retrieval Augmented Generation (RAG) is a paradigm that enhances large language models (LLMs) on knowledge-intensive tasks by combining their generative abilities with access to external knowledge. According to the context, a typical RAG framework comprises two core components: a retriever that selects relevant documents from a large corpus, and a generator that synthesizes information from these documents to produce coherent and contextually appropriate responses [2]. \n\nRAG works by retrieving relevant passages from an external database during inference time to ground responses in factual context, rather than relying solely on parametric memory [5]. This approach allows LLMs to efficiently utilize external knowledge for various applications, including code generation, question answering, and domain-specific tasks like pharmaceutical regulatory compliance [1, 3].\n\nThe paradigm has demonstrated strong performance across diverse domains including open-domain question answering, fact-checking, and summarization, making it particularly valuable for scenarios where LLMs need to incorporate external information to improve accuracy and relevance [2].",
        "answer_word_count": 153,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2402.12317v2",
          "2502.13957v2",
          "2402.01717v1",
          "2507.23334v2"
        ],
        "expected_ids": [
          "2601.05264v1",
          "2502.13957v2",
          "2401.15391v1",
          "2502.00306v2"
        ],
        "is_substantive": true,
        "latency_seconds": 16.79
      },
      {
        "question": "How can RAG systems be evaluated?",
        "topic": "rag_eval",
        "answer": "Based on the provided context, RAG systems can be evaluated through several approaches:\n\n1. **Automated reference-free evaluation frameworks** like Ragas (Paper [1], [2]) that assess:\n   - Faithfulness (is the answer grounded in retrieved context)\n   - Answer relevance (does the answer address the question)\n   - Context relevance (is retrieved context sufficiently focused)\n\n2. **User-centric evaluation** approaches that combine:\n   - Robustness-aware aggregation of normalized automatic metrics\n   - Human judgments based on ordinal likert-scale ratings\n   - Both static and real-time dynamic (RAG-arena) evaluation modes (Paper [4])\n\n3. **Specialized benchmarking datasets** such as:\n   - WikiEval (Paper [2]) with human judgments of faithfulness, answer relevance, and context relevance\n   - Multi-hop query datasets that evaluate retrieval accuracy for complex information integration tasks (Paper [3])\n\n4. **Language modeling tasks** like measuring perplexity on reference corpora (Paper [1]), though this approach may not always predict downstream performance.\n\n5. **Standardized evaluation frameworks** such as Ragchecker that provide precise diagnostic capabilities for systematic comparison and enhancement of RAG performance (Paper [5]).\n\nThe most comprehensive approach highlighted in the context is Ragas, which provides an automated assessment framework that works even in the absence of ground truth answers, with predictions closely aligned with human judgments, particularly for faithfulness and answer relevance (Paper [2]).",
        "answer_word_count": 202,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.15217v2",
          "2401.15391v1",
          "2602.20735v1",
          "2601.05264v1"
        ],
        "expected_ids": [
          "2309.15217v2",
          "2502.00306v2",
          "2601.05264v1"
        ],
        "is_substantive": true,
        "latency_seconds": 21.09
      },
      {
        "question": "What techniques exist to mitigate hallucinations in LLMs?",
        "topic": "hallucination",
        "answer": "# Techniques to Mitigate Hallucinations in LLMs\n\nBased on the provided context, the following techniques exist to mitigate hallucinations in LLMs:\n\n1. **RAG-based methods**: Lightweight retrieval-augmented generation approaches that integrate parametric knowledge with retrieved sources to balance internal reasoning and external evidence. Rowen (Paper [4]) demonstrates significant improvements on TruthfulQA (59.34% GPT-Judge score) and StrategyQA (75.60% accuracy).\n\n2. **Self-monitoring prompting frameworks**: Jha et al.'s approach uses formal methods to identify errors in LLM responses autonomously through iterative refinement (Paper [2]).\n\n3. **Self-familiarity**: A zero-resource pre-detection method that extracts and processes conceptual entities from instructions, then uses prompt engineering to acquire familiarity scores that indicate the likelihood of erroneous information generation (Paper [2]).\n\n4. **Consistency-based approaches**: Aggregating multiple responses from a single LLM for a given prompt to offset limitations in training data (Paper [3]). This can be extended to \"consortium consistency\" by combining responses from multiple LLMs with different training data and architectures.\n\n5. **Context-tagged prompts**: Feldman et al.'s method creates questions and context prompts to help LLMs answer more accurately, then validates the prompts to ensure effectiveness (Paper [2]).\n\n6. **Adaptive retrieval strategies**: Techniques that assess question difficulty or response confidence to decide whether to retrieve documents, maximizing the utilization of both parametric knowledge and external information (Paper [5]).\n\n7. **Multi-agent systems**: For iterative processes that include (i) clarifying task requirements, (ii) generating code, (iii) running test cases, and (iv) mitigating hallucinations (Paper [1]).\n\n8. **Input query refinement**: Techniques to improve the initial prompt to reduce hallucination potential (Paper [1]).",
        "answer_word_count": 251,
        "keyword_coverage": 0.2,
        "keywords_found": 1,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2409.20550v2",
          "2311.08117v1",
          "2510.19507v2",
          "2402.10612v3"
        ],
        "expected_ids": [
          "2311.08117v1",
          "2409.20550v2",
          "2507.15903v1",
          "2510.19507v2",
          "2502.11306v1"
        ],
        "is_substantive": true,
        "latency_seconds": 23.85
      },
      {
        "question": "What is instruction tuning and why is it important?",
        "topic": "instruction_tuning",
        "answer": "Based on the provided context, instruction tuning is a technique for training large language models (LLMs) to follow specific instructions or respond to user queries in a desired way. \n\nAccording to Paper [1], instruction tuning \"significantly enhances the performance of large language models (LLMs) across various tasks.\" Paper [3] states that instruction tuning was \"initially designed to enhance the generalization capabilities of LMs across different tasks via multitask training.\" Additionally, Paper [5] indicates that instruction tuning enables \"zero-shot capabilities\" - the ability for models to handle new tasks without specific training.\n\nInstruction tuning is important because it significantly improves LLM performance across diverse applications, enhances generalization capabilities across different tasks, and enables models to adapt to novel scenarios without requiring specific training for each new task.",
        "answer_word_count": 126,
        "keyword_coverage": 0.6,
        "keywords_found": 3,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2312.10793v3",
          "2409.14254v1",
          "2306.08568v2",
          "2405.19744v1",
          "2310.04793v2"
        ],
        "expected_ids": [
          "2304.07995v1",
          "2409.14254v1",
          "2304.03277v1",
          "2304.12244v3"
        ],
        "is_substantive": true,
        "latency_seconds": 33.31
      },
      {
        "question": "How does LoRA+ improve upon standard LoRA?",
        "topic": "lora_plus",
        "answer": "Based on the provided context, LoRA+ improves upon standard LoRA by addressing a fundamental limitation in the original LoRA implementation. \n\nAccording to Paper [2] (\"LoRA+: Efficient Low Rank Adaptation of Large Models\"), standard LoRA leads to suboptimal fine-tuning of models with large width (embedding dimension) because the adapter matrices A and B in LoRA are updated with the same learning rate. This uniform learning rate approach does not allow efficient feature learning in large models. \n\nLoRA+ corrects this suboptimality by setting different learning rates for the two low-rank adapter matrices with a well-chosen fixed ratio. This simple modification results in:\n- 1-2% improvements in performance\n- Up to 2x speedup in fine-tuning\n- Maintaining the same computational cost as standard LoRA\n\nThe paper demonstrates that this approach works because using scaling arguments for large width networks shows that the uniform learning rate in standard LoRA creates inefficiencies that can be resolved with this targeted learning rate adjustment.\n\nThis improvement is specifically designed for large models where the original LoRA implementation would otherwise be suboptimal due to the high embedding dimensions.",
        "answer_word_count": 180,
        "keyword_coverage": 0.4,
        "keywords_found": 2,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2407.18242v3",
          "2402.12354v2",
          "2506.20629v1",
          "2602.05988v1",
          "2106.09685v2"
        ],
        "expected_ids": [
          "2402.12354v2"
        ],
        "is_substantive": true,
        "latency_seconds": 17.46
      },
      {
        "question": "What is the difference between full fine-tuning and parameter-efficient fine-tuning?",
        "topic": "peft",
        "answer": "# Difference between full fine-tuning and parameter-efficient fine-tuning\n\nFull fine-tuning updates all parameters of a pretrained neural network when adapting it to a new task or domain (Paper [2]). It is described as \"the standard and most flexible method for adapting large models,\" particularly effective when the target task differs significantly from pretraining or when there is strong domain shift (Paper [2]).\n\nParameter-efficient fine-tuning (PEFT) methods, such as LoRA (Low-Rank Adaptation), update only a subset of parameters. LoRA \"reduces the number of trainable parameters from m Ã— n to (r + 1)\" where r is the rank parameter (Paper [3]). These methods provide \"substantial efficiency gain by reducing the memory usage by around 15%\" even when selecting only a few layers (Paper [4]).\n\nKey differences:\n1. **Parameters updated**: Full fine-tuning updates all parameters; PEFT updates only a subset (low-rank matrices or selected layers)\n2. **Computational requirements**: Full fine-tuning is \"memory- and compute-intensive\" while PEFT methods are more suitable for edge deployment (Paper [2])\n3. **Performance trade-off**: \"Despite its efficiency, LoRA's fine-tuning performance often falls short compared to full fine-tuning\" (Paper [3])\n4. **Scalability**: Full fine-tuning \"rarely suited for edge deployment\" due to high memory requirements, while PEFT enables \"large-scale personalization\" (Paper [2])\n\nThe most significant practical difference is that PEFT methods reduce computational and memory requirements while maintaining acceptable performance, making them suitable for resource-constrained environments where full fine-tuning would be impractical.",
        "answer_word_count": 232,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2110.06500v2",
          "2512.17983v1",
          "2407.18242v3",
          "2602.05988v1",
          "2602.08239v1"
        ],
        "expected_ids": [
          "2106.09685v2",
          "2305.14314v1",
          "2512.17983v1",
          "2412.09827v1"
        ],
        "is_substantive": true,
        "latency_seconds": 26.38
      },
      {
        "question": "How do multi-hop questions challenge RAG systems?",
        "topic": "multihop_rag",
        "answer": "Based on the provided context, multi-hop questions challenge RAG systems in several key ways:\n\n1. Retrieval challenges: The context states that \"the low values of the retrieval metrics highlight the challenges in retrieving relevant pieces of evidence for multi-hop queries when using direct similarity matching between the multihop query and text chunks\" (Paper: MultiHop-RAG).\n\n2. Reasoning limitations: Existing RAG systems are \"inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence\" (Paper: MultiHop-RAG).\n\n3. Performance issues: Both experiments in the MultiHop-RAG paper reveal that \"existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries\" (Paper: MultiHop-RAG).\n\n4. Evidence noise: \"Using the retrieved information directly as input incorporates some noises and makes the LLM lost in the useful information\" (Paper: RAG-Star).\n\n5. Model capability constraints: \"Relatively smaller models show limited performance on these knowledge-intensive reasoning tasks\" (Paper: RAG-Star).\n\nThe context also notes that no existing RAG benchmarking dataset focuses specifically on multi-hop queries, highlighting the gap in evaluation frameworks for this challenging task type (Paper: MultiHop-RAG).",
        "answer_word_count": 172,
        "keyword_coverage": 0.6,
        "keywords_found": 3,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2401.15391v1",
          "2502.13957v2",
          "2412.12881v1"
        ],
        "expected_ids": [
          "2401.15391v1",
          "2502.13957v2"
        ],
        "is_substantive": true,
        "latency_seconds": 21.22
      },
      {
        "question": "What is the role of vector databases in RAG pipelines?",
        "topic": "vector_db",
        "answer": "Based on the provided context, vector databases play a critical role in RAG pipelines by:\n\n1. Storing dense vector representations of text documents (after preprocessing) that capture semantic meaning for similarity search\n2. Enabling efficient retrieval of relevant contextual information from large knowledge bases by performing similarity searches\n3. Filtering information irrelevant to user queries while preserving context length (critical since modern LLMs have limited context capacity)\n4. Supporting periodic updates with new or revised data sources to maintain system accuracy as information evolves\n\nSpecifically, Paper [5] states: \"after preprocessing, every document chunk is converted into dense vector representations using medical domain-specific embedding models (e.g., e5-large-v2) that capture medical semantics more effectively than general-purpose embeddings. The resulting embedding vectors, along with their associated metadata, are stored in a vector database (such as pinecone)\".\n\nPaper [4] further explains that vector databases \"provide valuable context while preserving context length by filtering information irrelevant to the user's query,\" which is particularly important for modern AI models with limited context capacity.\n\nVector databases serve as the retrieval component that connects the LLM generation module with the knowledge base in RAG systems, enabling the system to provide contextually relevant responses while reducing the risk of hallucinations.",
        "answer_word_count": 201,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2505.12524v1",
          "2309.15217v2",
          "2601.05264v1",
          "2306.15295v2",
          "2505.03406v1"
        ],
        "expected_ids": [
          "2601.05264v1",
          "2505.12524v1",
          "2502.00306v2"
        ],
        "is_substantive": true,
        "latency_seconds": 19.31
      },
      {
        "question": "How can prompt engineering improve LLM outputs?",
        "topic": "prompt_engineering",
        "answer": "Based on the provided context, prompt engineering improves LLM outputs through several specific techniques:\n\n1. Zero-shot prompting: Providing only the task description to the LLM (Paper [1])\n\n2. Few-shot prompting: Giving the LLM a few examples of desired output as part of the prompt, which has been observed to improve output quality compared to alternative methods (Paper [2])\n\n3. Chain-of-thought reasoning: Asking the LLM to explain its reasoning step by step, which consistently improves outputs and helps prevent undesirable behaviors like hallucination (Paper [1])\n\n4. Improved prompt structure and context: Research shows that both \"context\" and \"structure\" of prompts significantly impact the quality of generated outputs (Paper [2])\n\n5. Reducing reasoning mistakes: Prompt engineering provides a lightweight method for influencing LLM behavior that has been shown to reduce reasoning mistakes and improve performance in various areas (Paper [3])\n\nThese techniques work by guiding the LLM toward desired outcomes without modifying the underlying model parameters, making prompt engineering an effective approach for improving output quality while maintaining the model's original capabilities.",
        "answer_word_count": 170,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.13734v2",
          "2507.03405v1",
          "2502.06039v1",
          "2503.02400v2"
        ],
        "expected_ids": [
          "2507.03405v1",
          "2309.13734v2",
          "2503.02400v2",
          "2401.14043v3"
        ],
        "is_substantive": true,
        "latency_seconds": 17.67
      },
      {
        "question": "What is Double Quantization in QLoRA?",
        "topic": "double_quant",
        "answer": "Double Quantization in QLoRA is a method that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model). This technique is part of QLoRA's approach to reduce memory usage without sacrificing performance.\n\nThe implementation involves two levels of quantization constants (c1 and c2), where the pre-trained weight matrix is preserved in 4-bit normalfloat (nf4) format. During inference, the doubledequantization process combines these levels to reconstruct the original weights from the quantized representation.\n\nThis information is primarily from \"QLoRA: Efficient Finetuning of Quantized LLMs\" (Paper [1]), with technical implementation details also described in \"Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding\" (Paper [3]).",
        "answer_word_count": 114,
        "keyword_coverage": 0.6,
        "keywords_found": 3,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2305.14314v1",
          "2401.00503v1",
          "2404.00862v1",
          "2505.03406v1",
          "2506.03178v1"
        ],
        "expected_ids": [
          "2305.14314v1",
          "2404.00862v1"
        ],
        "is_substantive": true,
        "latency_seconds": 21.07
      },
      {
        "question": "How does the Ragas framework evaluate RAG pipelines?",
        "topic": "ragas",
        "answer": "Based on the provided context, the Ragas framework evaluates RAG pipelines through a suite of reference-free metrics that assess three key dimensions of RAG system quality without requiring ground truth human annotations.\n\nAccording to Paper [1], Ragas provides a framework for reference-free evaluation of RAG pipelines by implementing metrics that assess:\n- The retrieval system's ability to identify relevant and focused context passages\n- The LLM's ability to exploit retrieved passages faithfully\n- The quality of the generation itself\n\nPaper [4] specifically states that Ragas evaluates three critical aspects: faithfulness (is the answer grounded in the retrieved context), answer relevance (does the answer address the question), and context relevance (is the retrieved context sufficiently focused).\n\nPaper [2] further details that Ragas achieves comprehensive evaluation capabilities through four core metrics: faithfulness, answer relevancy, context precision, and context recall. The framework uses LLM-based judges to evaluate response quality without requiring ground truth labels, making it particularly valuable for faster evaluation cycles of RAG architectures.\n\nThe framework was validated against the WikiEval dataset, showing that its predictions closely align with human predictions, especially for faithfulness and answer relevance.",
        "answer_word_count": 185,
        "keyword_coverage": 1.0,
        "keywords_found": 5,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2309.15217v2",
          "2601.05264v1",
          "2502.13957v2",
          "2510.22344v1"
        ],
        "expected_ids": [
          "2309.15217v2",
          "2601.05264v1"
        ],
        "is_substantive": true,
        "latency_seconds": 15.89
      },
      {
        "question": "What are corpus poisoning attacks on RAG systems?",
        "topic": "rag_security",
        "answer": "# Corpus Poisoning Attacks on RAG Systems\n\nCorpus poisoning attacks on RAG systems refer to attacks where adversaries inject malicious documents into the knowledge base to manipulate system behavior. Based on the provided context:\n\nAccording to Paper [1] \"RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation,\" these attacks can occur in practical scenarios such as:\n- E-commerce websites where malicious sellers add poisoned documents to make their products retrievable for irrelevant queries to increase sales\n- Legal RAG systems where adversaries attempt to make documents with incorrect judgments retrievable with malicious intentions\n\nPaper [3] \"Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems\" categorizes corpus poisoning as a high complexity attack type with \"document injection\" as the mechanism, noting it poses significant risks in industries like healthcare (privacy violations), finance (market manipulation), and legal services (citation fraud).\n\nThe context also indicates that such attacks can be part of broader threat models where adversaries \"have the capability of injecting malicious documents into rag knowledge database\" as mentioned in Paper [2] \"Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation.\"\n\nThese attacks represent a critical vulnerability in RAG systems that requires specific defense mechanisms to prevent manipulation of the knowledge base.",
        "answer_word_count": 210,
        "keyword_coverage": 0.8333333333333334,
        "keywords_found": 5,
        "keywords_total": 6,
        "source_hit": true,
        "source_ids": [
          "2512.24268v1",
          "2502.00306v2",
          "2601.05264v1"
        ],
        "expected_ids": [
          "2512.24268v1"
        ],
        "is_substantive": true,
        "latency_seconds": 19.62
      },
      {
        "question": "How can small language models be made more efficient through fine-tuning?",
        "topic": "small_llm",
        "answer": "Based on the provided context, small language models can be made more efficient through fine-tuning using:\n\n1. **Contrastive fine-tuning** (Paper [1] and [3]): This technique enhances text embeddings by training models to distinguish between similar and dissimilar text pairs. The paper shows this approach significantly improves performance for smaller models like MiniCPM (achieving 56.33% performance gain).\n\n2. **Low-Rank Adaptation (LoRA)** (Paper [2], [3], and [4]): This parameter-efficient fine-tuning technique adjusts only a small set of parameters, dramatically reducing computational and storage costs while maintaining model performance.\n\n3. **Layer-wise LoRA fine-tuning** (Paper [4]): This approach selects only the most relevant layers for fine-tuning using LoRA, reducing trainable parameters by up to 50% while maintaining or even improving predictive performance across different model architectures.\n\nThese techniques specifically address the resource constraints of smaller language models by optimizing the fine-tuning process to maintain performance with minimal computational overhead.",
        "answer_word_count": 145,
        "keyword_coverage": 0.8,
        "keywords_found": 4,
        "keywords_total": 5,
        "source_hit": true,
        "source_ids": [
          "2408.00690v2",
          "2402.01717v1",
          "2602.05988v1",
          "2408.07888v2"
        ],
        "expected_ids": [
          "2408.00690v2",
          "2504.16584v1",
          "2305.14314v1"
        ],
        "is_substantive": true,
        "latency_seconds": 18.87
      }
    ]
  }
}