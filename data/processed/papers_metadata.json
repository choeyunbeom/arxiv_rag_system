{
  "collected_at": "2026-02-25T18:26:35.108712",
  "total_papers": 132,
  "queries": [
    "Retrieval Augmented Generation",
    "Large Language Model fine-tuning",
    "QLoRA",
    "LoRA low rank adaptation",
    "RAG evaluation",
    "vector database embedding",
    "LLM hallucination mitigation",
    "instruction tuning",
    "prompt engineering techniques",
    "small language model efficiency"
  ],
  "papers": [
    {
      "arxiv_id": "2506.06962v3",
      "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
      "abstract": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating knearest neighbor retrievals at the patch level. Unlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images, AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, enabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.",
      "authors": [
        "Jingyuan Qi",
        "Zhiyang Xu",
        "Qifan Wang",
        "Lifu Huang"
      ],
      "published": "2025-06-08T01:33:05+00:00",
      "updated": "2025-06-14T00:40:34+00:00",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06962v3",
      "primary_category": "cs.CV",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2506.06962v3.pdf"
    },
    {
      "arxiv_id": "2504.13684v1",
      "title": "Intelligent Interaction Strategies for Context-Aware Cognitive Augmentation",
      "abstract": "Human cognition is constrained by processing limitations, leading to cognitive overload and inefficiencies in knowledge synthesis and decision-making. Large Language Models (LLMs) present an opportunity for cognitive augmentation, but their current reactive nature limits their real-world applicability. This position paper explores the potential of context-aware cognitive augmentation, where LLMs dynamically adapt to users' cognitive states and task environments to provide appropriate support. Through a think-aloud study in an exhibition setting, we examine how individuals interact with multi-modal information and identify key cognitive challenges in structuring, retrieving, and applying knowledge. Our findings highlight the need for AI-driven cognitive support systems that integrate real-time contextual awareness, personalized reasoning assistance, and socially adaptive interactions. We propose a framework for AI augmentation that seamlessly transitions between real-time cognitive support and post-experience knowledge organization, contributing to the design of more effective human-centered AI systems.",
      "authors": [
        "Xiangrong",
        "Zhu",
        "Yuan Xu",
        "Tianjian Liu",
        "Jingwei Sun",
        "Yu Zhang",
        "Xin Tong"
      ],
      "published": "2025-04-18T13:35:21+00:00",
      "updated": "2025-04-18T13:35:21+00:00",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.13684v1",
      "primary_category": "cs.HC",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2504.13684v1.pdf"
    },
    {
      "arxiv_id": "2504.17204v1",
      "title": "Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment",
      "abstract": "Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits.",
      "authors": [
        "Chitralekha Gupta",
        "Hanjun Wu",
        "Praveen Sasikumar",
        "Shreyas Sridhar",
        "Priambudi Bagaskara",
        "Suranga Nanayakkara"
      ],
      "published": "2025-04-24T02:29:50+00:00",
      "updated": "2025-04-24T02:29:50+00:00",
      "categories": [
        "cs.HC",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.17204v1",
      "primary_category": "cs.HC",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2504.17204v1.pdf"
    },
    {
      "arxiv_id": "2504.14689v1",
      "title": "Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking",
      "abstract": "The recent rapid advancement of LLM-based AI systems has accelerated our search and production of information. While the advantages brought by these systems seemingly improve the performance or efficiency of human activities, they do not necessarily enhance human capabilities. Recent research has started to examine the impact of generative AI on individuals' cognitive abilities, especially critical thinking. Based on definitions of critical thinking across psychology and education, this position paper proposes the distinction between demonstrated and performed critical thinking in the era of generative AI and discusses the implication of this distinction in research and development of AI systems that aim to augment human critical thinking.",
      "authors": [
        "Katelyn Xiaoying Mei",
        "Nic Weber"
      ],
      "published": "2025-04-20T17:40:28+00:00",
      "updated": "2025-04-20T17:40:28+00:00",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14689v1",
      "primary_category": "cs.HC",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2504.14689v1.pdf"
    },
    {
      "arxiv_id": "2411.18583v1",
      "title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
      "abstract": "This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). The ever-increasing number of research articles provides a huge challenge for manual literature review. It has resulted in an increased demand for automation. Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work. The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrieval-augmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. The ROUGE scores are used for the evaluation of all three systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo achieved the highest ROUGE-1 score, 0.364. The transformer model comes in second place and spaCy is at the last position. Finally, a graphical user interface is created for the best system based on the large language model.",
      "authors": [
        "Nurshat Fateh Ali",
        "Md. Mahdi Mohtasim",
        "Shakil Mosharrof",
        "T. Gopi Krishna"
      ],
      "published": "2024-11-27T18:27:07+00:00",
      "updated": "2024-11-27T18:27:07+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18583v1",
      "primary_category": "cs.CL",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2411.18583v1.pdf"
    },
    {
      "arxiv_id": "2402.12317v2",
      "title": "EVOR: Evolving Retrieval for Code Generation",
      "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research. Our model, code, and data are available at https://arks-codegen.github.io.",
      "authors": [
        "Hongjin Su",
        "Shuyang Jiang",
        "Yuhang Lai",
        "Haoyuan Wu",
        "Boao Shi",
        "Che Liu",
        "Qian Liu",
        "Tao Yu"
      ],
      "published": "2024-02-19T17:37:28+00:00",
      "updated": "2024-12-03T15:56:26+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12317v2",
      "primary_category": "cs.CL",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2402.12317v2.pdf"
    },
    {
      "arxiv_id": "2502.00306v2",
      "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.",
      "authors": [
        "Ali Naseh",
        "Yuefeng Peng",
        "Anshuman Suri",
        "Harsh Chaudhari",
        "Alina Oprea",
        "Amir Houmansadr"
      ],
      "published": "2025-02-01T04:01:18+00:00",
      "updated": "2025-06-30T16:37:59+00:00",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00306v2",
      "primary_category": "cs.CR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2502.00306v2.pdf"
    },
    {
      "arxiv_id": "2309.15217v2",
      "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation",
      "abstract": "We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \\textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",
      "authors": [
        "Shahul Es",
        "Jithin James",
        "Luis Espinosa-Anke",
        "Steven Schockaert"
      ],
      "published": "2023-09-26T19:23:54+00:00",
      "updated": "2025-04-28T05:09:12+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15217v2",
      "primary_category": "cs.CL",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2309.15217v2.pdf"
    },
    {
      "arxiv_id": "2507.23334v2",
      "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation",
      "abstract": "Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.",
      "authors": [
        "Daeyong Kwon",
        "SeungHeon Doh",
        "Juhan Nam"
      ],
      "published": "2025-07-31T08:31:05+00:00",
      "updated": "2025-12-08T08:08:34+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.23334v2",
      "primary_category": "cs.CL",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2507.23334v2.pdf"
    },
    {
      "arxiv_id": "2601.05264v1",
      "title": "Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems",
      "abstract": "This article provides a comprehensive systematic literature review of academic studies, industrial applications, and real-world deployments from 2018 to 2025, providing a practical guide and detailed overview of modern Retrieval-Augmented Generation (RAG) architectures. RAG offers a modular approach for integrating external knowledge without increasing the capacity of the model as LLM systems expand. Research and engineering practices have been fragmented as a result of the increasing diversity of RAG methodologies, which encompasses a variety of fusion mechanisms, retrieval strategies, and orchestration approaches. We provide quantitative assessment frameworks, analyze the implications for trust and alignment, and systematically consolidate existing RAG techniques into a unified taxonomy. This document is a practical framework for the deployment of resilient, secure, and domain-adaptable RAG systems, synthesizing insights from academic literature, industry reports, and technical implementation guides. It also functions as a technical reference.",
      "authors": [
        "Dean Wampler",
        "Dave Nielson",
        "Alireza Seddighi"
      ],
      "published": "2025-11-07T16:26:29+00:00",
      "updated": "2025-11-07T16:26:29+00:00",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.05264v1",
      "primary_category": "cs.IR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2601.05264v1.pdf"
    },
    {
      "arxiv_id": "2510.22344v1",
      "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation",
      "abstract": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
      "authors": [
        "Mohammad Aghajani Asl",
        "Majid Asgari-Bidhendi",
        "Behrooz Minaei-Bidgoli"
      ],
      "published": "2025-10-25T15:59:33+00:00",
      "updated": "2025-10-25T15:59:33+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.22344v1",
      "primary_category": "cs.CL",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2510.22344v1.pdf"
    },
    {
      "arxiv_id": "2601.11863v1",
      "title": "Utilizing Metadata for Better Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.",
      "authors": [
        "Raquib Bin Yousuf",
        "Shengzhe Xu",
        "Mandar Sharma",
        "Andrew Neeser",
        "Chris Latimer",
        "Naren Ramakrishnan"
      ],
      "published": "2026-01-17T01:11:03+00:00",
      "updated": "2026-01-17T01:11:03+00:00",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.11863v1",
      "primary_category": "cs.IR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2601.11863v1.pdf"
    },
    {
      "arxiv_id": "1811.08772v1",
      "title": "Overcoming low-utility facets for complex answer retrieval",
      "abstract": "Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Complex Answer Retrieval (CAR) is the retrieval of answers to such questions. In their simplest form, these questions are constructed from a topic entity (e.g., `cheese') and a facet (e.g., `health effects'). While topic matching has been thoroughly explored, we observe that some facets use general language that is unlikely to appear verbatim in answers. We call these low-utility facets. In this work, we present an approach to CAR that identifies and addresses low-utility facets. We propose two estimators of facet utility. These include exploiting the hierarchical structure of CAR queries and using facet frequency information from training data. To improve the retrieval performance on low-utility headings, we also include entity similarity scores using knowledge graph embeddings. We apply our approaches to a leading neural ranking technique, and evaluate using the TREC CAR dataset. We find that our approach perform significantly better than the unmodified neural ranker and other leading CAR techniques. We also provide a detailed analysis of our results, and verify that low-utility facets are indeed more difficult to match, and that our approach improves the performance for these difficult queries.",
      "authors": [
        "Sean MacAvaney",
        "Andrew Yates",
        "Arman Cohan",
        "Luca Soldaini",
        "Kai Hui",
        "Nazli Goharian",
        "Ophir Frieder"
      ],
      "published": "2018-11-21T15:09:00+00:00",
      "updated": "2018-11-21T15:09:00+00:00",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/1811.08772v1",
      "primary_category": "cs.IR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/1811.08772v1.pdf"
    },
    {
      "arxiv_id": "2512.24268v1",
      "title": "RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.",
      "authors": [
        "Pankayaraj Pathmanathan",
        "Michael-Andrei Panaitescu-Liess",
        "Cho-Yu Jason Chiang",
        "Furong Huang"
      ],
      "published": "2025-12-30T14:43:57+00:00",
      "updated": "2025-12-30T14:43:57+00:00",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.24268v1",
      "primary_category": "cs.IR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2512.24268v1.pdf"
    },
    {
      "arxiv_id": "2602.07525v1",
      "title": "IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory",
      "abstract": "Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.",
      "authors": [
        "Xingliang Hou",
        "Yuyan Liu",
        "Qi Sun",
        "haoxiu wang",
        "Hao Hu",
        "Shaoyi Du",
        "Zhiqiang Tian"
      ],
      "published": "2026-02-07T12:42:31+00:00",
      "updated": "2026-02-07T12:42:31+00:00",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.07525v1",
      "primary_category": "cs.IR",
      "query_source": "Retrieval Augmented Generation",
      "local_pdf_path": "data/raw/2602.07525v1.pdf"
    },
    {
      "arxiv_id": "2312.10793v3",
      "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
      "abstract": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.",
      "authors": [
        "Renxi Wang",
        "Haonan Li",
        "Minghao Wu",
        "Yuxia Wang",
        "Xudong Han",
        "Chiyu Zhang",
        "Timothy Baldwin"
      ],
      "published": "2023-12-17T18:44:26+00:00",
      "updated": "2024-02-18T17:13:36+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.10793v3",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2312.10793v3.pdf"
    },
    {
      "arxiv_id": "2110.06500v2",
      "title": "Differentially Private Fine-tuning of Language Models",
      "abstract": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $ε= 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8 respectively (privacy budget of $ε= 6.8,δ=$ 1e-5) whereas the non-private baseline is $48.1$. All our experiments suggest that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.",
      "authors": [
        "Da Yu",
        "Saurabh Naik",
        "Arturs Backurs",
        "Sivakanth Gopi",
        "Huseyin A. Inan",
        "Gautam Kamath",
        "Janardhan Kulkarni",
        "Yin Tat Lee",
        "Andre Manoel",
        "Lukas Wutschitz",
        "Sergey Yekhanin",
        "Huishuai Zhang"
      ],
      "published": "2021-10-13T05:15:00+00:00",
      "updated": "2022-07-14T22:14:17+00:00",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CR",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2110.06500v2",
      "primary_category": "cs.LG",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2110.06500v2.pdf"
    },
    {
      "arxiv_id": "2402.11651v2",
      "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
      "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.",
      "authors": [
        "Renxi Wang",
        "Haonan Li",
        "Xudong Han",
        "Yixuan Zhang",
        "Timothy Baldwin"
      ],
      "published": "2024-02-18T17:10:07+00:00",
      "updated": "2024-04-16T11:41:13+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.11651v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2402.11651v2.pdf"
    },
    {
      "arxiv_id": "2602.08239v1",
      "title": "Linearization Explains Fine-Tuning in Large Language Models",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.",
      "authors": [
        "Zahra Rahimi Afzal",
        "Tara Esmaeilbeig",
        "Mojtaba Soltanalian",
        "Mesrob I. Ohannessian"
      ],
      "published": "2026-02-09T03:27:58+00:00",
      "updated": "2026-02-09T03:27:58+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.08239v1",
      "primary_category": "cs.LG",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2602.08239v1.pdf"
    },
    {
      "arxiv_id": "2403.00946v3",
      "title": "Fine-tuning with Very Large Dropout",
      "abstract": "It is impossible today to pretend that the practice of machine learning is always compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.\n  This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods such as model soups.\n  This result has practical significance because the importance of the fine-tuning scenario has considerably grown in recent years. This result also provides interesting insights on the nature of rich representations and on the intrinsically linear nature of fine-tuning a large network using a comparatively small dataset.",
      "authors": [
        "Jianyu Zhang",
        "Léon Bottou"
      ],
      "published": "2024-03-01T19:50:22+00:00",
      "updated": "2025-02-27T22:15:53+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.00946v3",
      "primary_category": "cs.LG",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2403.00946v3.pdf"
    },
    {
      "arxiv_id": "2304.12244v3",
      "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
      "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
      "authors": [
        "Can Xu",
        "Qingfeng Sun",
        "Kai Zheng",
        "Xiubo Geng",
        "Pu Zhao",
        "Jiazhan Feng",
        "Chongyang Tao",
        "Qingwei Lin",
        "Daxin Jiang"
      ],
      "published": "2023-04-24T16:31:06+00:00",
      "updated": "2025-05-27T06:49:09+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12244v3",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2304.12244v3.pdf"
    },
    {
      "arxiv_id": "2508.04848v1",
      "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning",
      "abstract": "Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX.",
      "authors": [
        "Chang Tian",
        "Matthew B. Blaschko",
        "Mingzhe Xing",
        "Xiuxing Li",
        "Yinliang Yue",
        "Marie-Francine Moens"
      ],
      "published": "2025-08-06T19:51:29+00:00",
      "updated": "2025-08-06T19:51:29+00:00",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.04848v1",
      "primary_category": "cs.AI",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2508.04848v1.pdf"
    },
    {
      "arxiv_id": "2210.12607v1",
      "title": "Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models",
      "abstract": "How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over end-to-end learning that focuses on the target task alone. However, chain of thought prompting has significant limitations due to its dependency on huge pretrained LMs. In this work, we present compositional fine-tuning (CFT): an approach based on explicitly decomposing a target task into component tasks, and then fine-tuning smaller LMs on a curriculum of such component tasks. We apply CFT to recommendation tasks in two domains, world travel and local dining, as well as a previously studied inferential task (sports understanding). We show that CFT outperforms end-to-end learning even with equal amounts of data, and gets consistently better as more component tasks are modeled via fine-tuning. Compared with chain of thought prompting, CFT performs at least as well using LMs only 7.4% of the size, and is moreover applicable to task domains for which data are not available during pretraining.",
      "authors": [
        "Victor S. Bursztyn",
        "David Demeter",
        "Doug Downey",
        "Larry Birnbaum"
      ],
      "published": "2022-10-23T03:22:34+00:00",
      "updated": "2022-10-23T03:22:34+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12607v1",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2210.12607v1.pdf"
    },
    {
      "arxiv_id": "2508.11281v2",
      "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection",
      "abstract": "Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, human-annotated, large-scale datasets. In this work, we release ToxiFrench, a dataset of 53,622 French online comments together with a balanced benchmark split for systematic evaluation. The dataset is constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification, while ensuring statistical alignment with human-only annotation. We then benchmark a broad range of models and uncover a counterintuitive finding: Small Language Models (SLMs) often surpass larger models in robustness and generalization on this task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a Dynamic Weighted Loss (DWL) that progressively emphasizes the model's final decision and significantly improves faithfulness. Our fine-tuned 4B model (Qwen3-4B) achieves state-of-the-art performance on the benchmark. It improves its balanced accuracy by 10% over its baseline and achieves better performance than GPT-4o and DeepSeek-R1 on our benchmark, while successfully retaining cross-lingual capabilities.",
      "authors": [
        "Axel Delaval",
        "Shujian Yang",
        "Haicheng Wang",
        "Han Qiu",
        "Jialiang Lu"
      ],
      "published": "2025-08-15T07:40:41+00:00",
      "updated": "2026-01-19T15:55:38+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.11281v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2508.11281v2.pdf"
    },
    {
      "arxiv_id": "2504.16584v1",
      "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code",
      "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.",
      "authors": [
        "Md. Azizul Hakim Bappy",
        "Hossen A Mustafa",
        "Prottoy Saha",
        "Rajinus Salehat"
      ],
      "published": "2025-04-23T10:05:27+00:00",
      "updated": "2025-04-23T10:05:27+00:00",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.16584v1",
      "primary_category": "cs.CR",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2504.16584v1.pdf"
    },
    {
      "arxiv_id": "2309.13734v2",
      "title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
      "abstract": "Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance. In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for. The code used in this study is available at \\url{https://github.com/ijcruic/LLM-Stance-Labeling}",
      "authors": [
        "Iain J. Cruickshank",
        "Lynnette Hui Xian Ng"
      ],
      "published": "2023-09-24T19:36:17+00:00",
      "updated": "2024-03-05T21:26:54+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13734v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2309.13734v2.pdf"
    },
    {
      "arxiv_id": "2408.00690v2",
      "title": "Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning",
      "abstract": "While Large Language Models show remarkable performance in natural language understanding, their resource-intensive nature makes them less accessible. In contrast, smaller language models such as MiniCPM offer more sustainable scalability, but often underperform without specialized optimization. In this paper, we explore the enhancement of smaller language models through the improvement of their text embeddings. We select three language models, MiniCPM, Phi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our results demonstrate that this fine-tuning method enhances the quality of text embeddings for all three models across various benchmarks, with MiniCPM showing the most significant improvements of an average 56.33% performance gain. The contrastive fine-tuning code is publicly available at https://github.com/trapoom555/Language-Model-STS-CFT.",
      "authors": [
        "Trapoom Ukarapol",
        "Zhicheng Lee",
        "Amy Xin"
      ],
      "published": "2024-08-01T16:31:35+00:00",
      "updated": "2024-08-02T14:36:05+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00690v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2408.00690v2.pdf"
    },
    {
      "arxiv_id": "2408.07888v2",
      "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
      "abstract": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs, driving the need for data-efficient training with optimised data ordering. Human-inspired strategies offer a solution by organising data based on human learning practices. This study evaluates the fine-tuning efficiency of five human-inspired strategies across four language models, three datasets, and both human- and LLM-labelled data in the context of medical question answering. These strategies achieve the best accuracy gain of 1.81% and an average gain of 1.02% across datasets, with interleaved strategies delivering the best average results. However, the best strategy varies across model-dataset combinations, limiting the generalisability of the effects of any single strategy. Additionally, LLM-defined question difficulty outperforms human-defined labels in curriculum-based learning, showing the potential of model-generated data as a cost-effective alternative for optimising fine-tuning.",
      "authors": [
        "Yushi Yang",
        "Andrew M. Bean",
        "Robert McCraith",
        "Adam Mahdi"
      ],
      "published": "2024-08-15T02:22:48+00:00",
      "updated": "2024-11-05T11:07:19+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07888v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2408.07888v2.pdf"
    },
    {
      "arxiv_id": "2306.08568v2",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
      "authors": [
        "Ziyang Luo",
        "Can Xu",
        "Pu Zhao",
        "Qingfeng Sun",
        "Xiubo Geng",
        "Wenxiang Hu",
        "Chongyang Tao",
        "Jing Ma",
        "Qingwei Lin",
        "Daxin Jiang"
      ],
      "published": "2023-06-14T15:18:48+00:00",
      "updated": "2025-05-27T07:40:36+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08568v2",
      "primary_category": "cs.CL",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2306.08568v2.pdf"
    },
    {
      "arxiv_id": "2312.00249v2",
      "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
      "abstract": "The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as the inputs to the language model. To mitigate data scarcity in the audio domain, a curriculum learning strategy is proposed by formulating diverse audio tasks in a sequential manner. Moreover, we improve the audio language model by using interleaved audio-text embeddings as the input sequence. In this improved model, zero constraints are imposed on the input format, thus it is capable of tackling diverse modelling tasks, such as few-shot audio classification and audio comparison. To further evaluate the advanced ability of the audio networks, we introduce natural language audio reasoning (NLAR), a new task that analyses two audio clips by comparison and summarisation. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the target datasets) across various tasks. We finally demonstrate APT's ability in extending frozen VLMs to the audio domain without fine-tuning, achieving promising results in audio-visual question and answering. Our code and model weights will be released at https://github.com/JinhuaLiang/APT",
      "authors": [
        "Jinhua Liang",
        "Xubo Liu",
        "Wenwu Wang",
        "Mark D. Plumbley",
        "Huy Phan",
        "Emmanouil Benetos"
      ],
      "published": "2023-11-30T23:43:59+00:00",
      "updated": "2025-02-18T09:42:14+00:00",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.00249v2",
      "primary_category": "eess.AS",
      "query_source": "Large Language Model fine-tuning",
      "local_pdf_path": "data/raw/2312.00249v2.pdf"
    },
    {
      "arxiv_id": "2505.07877v1",
      "title": "Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data",
      "abstract": "General-purpose large language models (LLMs), despite their broad capabilities accrued from open-world data, frequently exhibit suboptimal performance when confronted with the nuanced and specialized demands inherent in real-time telecommunications applications. This investigation addresses this critical limitation through the meticulous fine-tuning of TSLAM-Mini developed by NetoAI, a compact (3.8-billion parameter) causal language model architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen leverages a bespoke dataset comprising 100,000 samples, strategically engineered to address 20 pivotal telecommunications use-cases, encompassing domains such as Network Fundamentals, IP Routing, MPLS, Network Security, Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched with granular insights from venerated network Subject Matter Experts (SMEs) and authoritative RFC documents, thereby capturing high-fidelity representations of real-world network dynamics through simulations inspired by digital twin paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial training efficiency and enabled prospective deployment on resource-constrained hardware. A novel evaluation framework, predicated on a high-capacity LLM (Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to rigorously assess instruction-following fidelity and response quality across the specified telecom use-cases. Empirical results unequivocally demonstrate TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring the profound efficacy of domain-specific datasets and PEFT methodologies for advancing intelligent network management.",
      "authors": [
        "Vignesh Ethiraj",
        "Divya Vijay",
        "Sidhanth Menon",
        "Heblin Berscilla"
      ],
      "published": "2025-05-10T12:28:47+00:00",
      "updated": "2025-05-10T12:28:47+00:00",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.07877v1",
      "primary_category": "cs.NI",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2505.07877v1.pdf"
    },
    {
      "arxiv_id": "2305.14314v1",
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
      "authors": [
        "Tim Dettmers",
        "Artidoro Pagnoni",
        "Ari Holtzman",
        "Luke Zettlemoyer"
      ],
      "published": "2023-05-23T17:50:33+00:00",
      "updated": "2023-05-23T17:50:33+00:00",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14314v1",
      "primary_category": "cs.LG",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2305.14314v1.pdf"
    },
    {
      "arxiv_id": "2502.10202v1",
      "title": "Can Post-Training Quantization Benefit from an Additional QLoRA Integration?",
      "abstract": "Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.",
      "authors": [
        "Xiliang Zhu",
        "Elena Khasanova",
        "Cheng Chen"
      ],
      "published": "2025-02-14T14:56:19+00:00",
      "updated": "2025-02-14T14:56:19+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.10202v1",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2502.10202v1.pdf"
    },
    {
      "arxiv_id": "2511.19493v1",
      "title": "RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression",
      "abstract": "RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.\n  This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.\n  Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.",
      "authors": [
        "Chris Kuchar"
      ],
      "published": "2025-11-23T12:00:33+00:00",
      "updated": "2025-11-23T12:00:33+00:00",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.19493v1",
      "primary_category": "cs.LG",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2511.19493v1.pdf"
    },
    {
      "arxiv_id": "2512.17983v1",
      "title": "Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models",
      "abstract": "Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.",
      "authors": [
        "Irina Seregina",
        "Philippe Lalanda",
        "German Vega"
      ],
      "published": "2025-12-19T14:12:43+00:00",
      "updated": "2025-12-19T14:12:43+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.17983v1",
      "primary_category": "cs.LG",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2512.17983v1.pdf"
    },
    {
      "arxiv_id": "2510.03683v2",
      "title": "Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text",
      "abstract": "The use of derogatory terms in languages that employ code mixing, such as Roman Urdu, presents challenges for Natural Language Processing systems due to unstated grammar, inconsistent spelling, and a scarcity of labeled data. In this work, we propose a QLoRA based fine tuning framework to improve offensive language detection in Roman Urdu-English text. We translated the Roman Urdu-English code mixed dataset into English using Google Translate to leverage English LLMs, while acknowledging that this translation reduces direct engagement with code mixing features. Our focus is on classification performance using English translated low resource inputs. We fine tuned several transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient adaptation. Models were trained and evaluated on a manually annotated Roman Urdu dataset for offensive vs non offensive content. Of all tested models, the highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral 7B at 89.66, surpassing traditional transformer baselines. These results demonstrate the efficacy of QLoRA in fine tuning high performing models for low resource environments such as code mixed offensive language detection, and confirm the potential of LLMs for this task. This work advances a scalable approach to Roman Urdu moderation and paves the way for future multilingual offensive detection systems based on LLMs.",
      "authors": [
        "Nisar Hussain",
        "Amna Qasim",
        "Gull Mehak",
        "Muhammad Zain",
        "Momina Hafeez",
        "Grigori Sidorov"
      ],
      "published": "2025-10-04T05:38:46+00:00",
      "updated": "2025-10-10T05:42:06+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.03683v2",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2510.03683v2.pdf"
    },
    {
      "arxiv_id": "2503.12988v1",
      "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
      "abstract": "As large language models (LLMs) demonstrate powerful capabilities, deploying them on edge devices has become increasingly crucial, offering advantages in privacy and real-time interaction. QLoRA has emerged as the standard approach for on-device LLMs, leveraging quantized models to reduce memory and computational costs while utilizing LoRA for task-specific adaptability. In this work, we propose ROMA, a QLoRA accelerator with a hybrid storage architecture that uses ROM for quantized base models and SRAM for LoRA weights and KV cache. Our insight is that the quantized base model is stable and converged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer the flexibility to adapt to new data without requiring updates to the base model. To further reduce the area cost of ROM, we introduce a novel B-ROM design and integrate it with the compute unit to form a fused cell for efficient use of chip resources. ROMA can effectively store both a 4-bit 3B and a 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed exceeding 20,000 tokens/s without requiring external memory.",
      "authors": [
        "Wenqiang Wang",
        "Yijia Zhang",
        "Zikai Zhang",
        "Guanting Huo",
        "Hao Liang",
        "Shijie Cao",
        "Ningyi Xu"
      ],
      "published": "2025-03-17T09:44:17+00:00",
      "updated": "2025-03-17T09:44:17+00:00",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12988v1",
      "primary_category": "cs.AR",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2503.12988v1.pdf"
    },
    {
      "arxiv_id": "2404.00862v1",
      "title": "Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding",
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chinese, we conduct secondary pre-training on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of benchmark datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other benchmark datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.",
      "authors": [
        "Lung-Chuan Chen",
        "Zong-Ru Li"
      ],
      "published": "2024-04-01T02:04:44+00:00",
      "updated": "2024-04-01T02:04:44+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00862v1",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2404.00862v1.pdf"
    },
    {
      "arxiv_id": "2406.08582v1",
      "title": "Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods",
      "abstract": "There are various methods for adapting LLMs to different domains. The most common methods are prompting, finetuning, and RAG. In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA. The experiment aims to simulate human responses based on their interviews. The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts.",
      "authors": [
        "Eugene Vyborov",
        "Oleksiy Osypenko",
        "Serge Sotnyk"
      ],
      "published": "2024-06-12T18:38:40+00:00",
      "updated": "2024-06-12T18:38:40+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08582v1",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2406.08582v1.pdf"
    },
    {
      "arxiv_id": "2509.12229v1",
      "title": "Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study",
      "abstract": "Fine-tuning large language models (LLMs) with parameter-efficient techniques such as LoRA and QLoRA has enabled adaptation of foundation models on modest hardware. Yet the efficiency of such training on consumer-grade GPUs, especially under strict 8 GB VRAM limits, remains underexplored. We present a controlled profiling study of LoRA/QLoRA fine-tuning using the Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three representative configurations, we systematically vary batch size, sequence length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16). We report throughput (tokens/s), time per 10k tokens, and VRAM footprint, alongside energy estimates derived from GPU board power limits. Our results show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500 tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB constraints, sequence lengths up to 2048 tokens were feasible using parameter-efficient strategies. To our knowledge, this is the first systematic case study of LLM fine- tuning efficiency on consumer GPUs, providing reproducible benchmarks and practical guidelines for resource-constrained researchers and practitioners.",
      "authors": [
        "MSR Avinash"
      ],
      "published": "2025-09-07T21:41:14+00:00",
      "updated": "2025-09-07T21:41:14+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.12229v1",
      "primary_category": "cs.LG",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2509.12229v1.pdf"
    },
    {
      "arxiv_id": "2505.03406v1",
      "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation",
      "abstract": "This research paper investigates the application of Large Language Models (LLMs) in healthcare, specifically focusing on enhancing medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation (QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By embedding and retrieving context-relevant healthcare information, the system significantly improves response accuracy. QLoRA facilitates notable parameter efficiency and memory optimization, preserving the integrity of medical information through specialized quantization techniques. Our research also shows that our model performs relatively well on various medical benchmarks, indicating that it can be used to make basic medical suggestions. This paper details the system's technical components, including its architecture, quantization methods, and key healthcare applications such as enhanced disease prediction from patient symptoms and medical history, treatment suggestions, and efficient summarization of complex medical reports. We touch on the ethical considerations-patient privacy, data security, and the need for rigorous clinical validation-as well as the practical challenges of integrating such systems into real-world healthcare workflows. Furthermore, the lightweight quantized weights ensure scalability and ease of deployment even in low-resource hospital environments. Finally, the paper concludes with an analysis of the broader impact of LLMs on healthcare and outlines future directions for LLMs in medical settings.",
      "authors": [
        "Mohammad Shoaib Ansari",
        "Mohd Sohail Ali Khan",
        "Shubham Revankar",
        "Aditya Varma",
        "Anil S. Mokhade"
      ],
      "published": "2025-05-06T10:31:54+00:00",
      "updated": "2025-05-06T10:31:54+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.03406v1",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2505.03406v1.pdf"
    },
    {
      "arxiv_id": "2309.09902v2",
      "title": "Speaker attribution in German parliamentary debates with QLoRA-adapted large language models",
      "abstract": "The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates. Our results shed light on the capabilities of large language models in automating speaker attribution, revealing a promising avenue for computational analysis of political discourse and the development of semantic role labeling systems.",
      "authors": [
        "Tobias Bornheim",
        "Niklas Grieger",
        "Patrick Gustav Blaneck",
        "Stephan Bialonski"
      ],
      "published": "2023-09-18T16:06:16+00:00",
      "updated": "2024-03-01T10:39:29+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09902v2",
      "primary_category": "cs.CL",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2309.09902v2.pdf"
    },
    {
      "arxiv_id": "2602.15836v1",
      "title": "EdgeNav-QE: QLoRA Quantization and Dynamic Early Exit for LAM-based Navigation on Edge Devices",
      "abstract": "Large Action Models (LAMs) have shown immense potential in autonomous navigation by bridging high-level reasoning with low-level control. However, deploying these multi-billion parameter models on edge devices remains a significant challenge due to memory constraints and latency requirements. In this paper, we propose EdgeNav-QE, a novel framework that integrates Quantized Low-Rank Adaptation (QLoRA) with a dynamic early-exit (DEE) mechanism to optimize LAMs for real-time edge navigation. By quantizing the backbone to 4-bit precision and strategically placing early-exit branches, we enable the model to terminate inference early for simple navigation tasks while retaining full depth for complex decision-making. Experimental results on the Habitat-Sim environment with Matterport3D dataset using OpenVLA-7B backbone, demonstrate that EdgeNav-QE reduces inference latency by 82.7% and memory footprint by 66.7% compared to full-precision baselines, while maintaining 81.8% navigation success rate. Furthermore, it outperforms state-of-the-art static early-exit method by 17.9% in latency, demonstrating the superiority of content-aware adaptive computation for safety-critical applications.",
      "authors": [
        "Mengyun Liu",
        "Shanshan Huang",
        "Jianan Jiang"
      ],
      "published": "2026-01-12T09:20:53+00:00",
      "updated": "2026-01-12T09:20:53+00:00",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.15836v1",
      "primary_category": "cs.RO",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2602.15836v1.pdf"
    },
    {
      "arxiv_id": "2506.03178v1",
      "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning",
      "abstract": "Automated radiology report generation holds significant potential to reduce radiologists' workload and enhance diagnostic accuracy. However, generating precise and clinically meaningful reports from chest radiographs remains challenging due to the complexity of medical language and the need for contextual understanding. Existing models often struggle with maintaining both accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves improved coherence and clinical accuracy while maintaining computational efficiency. This efficiency is driven by an optimization strategy that enhances parameter utilization and reduces memory overhead, enabling faster report generation with lower computational resource demands. Extensive experiments conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L score of 0.433 and a METEOR score of 0.336, establishing new performance benchmarks in the domain. These results underscore LLaMA-XR's potential as an effective and efficient AI system for automated radiology reporting, offering enhanced clinical utility and reliability.",
      "authors": [
        "Md. Zihad Bin Jahangir",
        "Muhammad Ashad Kabir",
        "Sumaiya Akter",
        "Israt Jahan",
        "Minh Chau"
      ],
      "published": "2025-05-29T12:21:18+00:00",
      "updated": "2025-05-29T12:21:18+00:00",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.03178v1",
      "primary_category": "eess.IV",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2506.03178v1.pdf"
    },
    {
      "arxiv_id": "2401.00503v1",
      "title": "Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI",
      "abstract": "This paper aims to introduce and analyze the Viz system in a comprehensive way, a novel system architecture that integrates Quantized Low-Rank Adapters (QLoRA) to fine-tune large language models (LLM) within a legally compliant and resource efficient marketplace. Viz represents a significant contribution to the field of artificial intelligence, particularly in addressing the challenges of computational efficiency, legal compliance, and economic sustainability in the utilization and monetization of LLMs. The paper delineates the scholarly discourse and developments that have informed the creation of Viz, focusing primarily on the advancements in LLM models, copyright issues in AI training (NYT case, 2023), and the evolution of model fine-tuning techniques, particularly low-rank adapters and quantized low-rank adapters, to create a sustainable and economically compliant framework for LLM utilization. The economic model it proposes benefits content creators, AI developers, and end-users, delineating a harmonious integration of technology, economy, and law, offering a comprehensive solution to the complex challenges of today's AI landscape.",
      "authors": [
        "Dipankar Sarkar"
      ],
      "published": "2023-12-31T13:53:06+00:00",
      "updated": "2023-12-31T13:53:06+00:00",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.00503v1",
      "primary_category": "cs.LG",
      "query_source": "QLoRA",
      "local_pdf_path": "data/raw/2401.00503v1.pdf"
    },
    {
      "arxiv_id": "2411.14961v3",
      "title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement",
      "abstract": "Foundation models (FMs) achieve strong performance across diverse tasks with task-specific fine-tuning, yet full parameter fine-tuning is often computationally prohibitive for large models. Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing low-rank matrices for tuning fewer parameters. While LoRA allows for efficient fine-tuning, it requires significant data for adaptation, making Federated Learning (FL) an appealing solution due to its privacy-preserving collaborative framework. However, combining LoRA with FL introduces two key challenges: the \\textbf{Server-Side Aggregation Bias}, where server-side averaging of LoRA matrices diverges from the ideal global update, and the \\textbf{Client-Side Initialization Lag}, emphasizing the need for consistent initialization across rounds. Existing approaches address these challenges individually, limiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles both issues by introducing a correction term on the server, enhancing aggregation efficiency and accuracy. LoRA-FAIR maintains computational and communication efficiency, yielding superior performance over state-of-the-art methods. Experimental results on ViT and MLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR consistently achieves performance improvements in FL settings.",
      "authors": [
        "Jieming Bian",
        "Lei Wang",
        "Letian Zhang",
        "Jie Xu"
      ],
      "published": "2024-11-22T14:19:01+00:00",
      "updated": "2025-10-12T17:37:47+00:00",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14961v3",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2411.14961v3.pdf"
    },
    {
      "arxiv_id": "1411.4510v1",
      "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation",
      "abstract": "The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.",
      "authors": [
        "Kian Hsiang Low",
        "Jiangbo Yu",
        "Jie Chen",
        "Patrick Jaillet"
      ],
      "published": "2014-11-17T15:31:04+00:00",
      "updated": "2014-11-17T15:31:04+00:00",
      "categories": [
        "stat.ML",
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1411.4510v1",
      "primary_category": "stat.ML",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/1411.4510v1.pdf"
    },
    {
      "arxiv_id": "2402.12354v2",
      "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
      "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.",
      "authors": [
        "Soufiane Hayou",
        "Nikhil Ghosh",
        "Bin Yu"
      ],
      "published": "2024-02-19T18:33:49+00:00",
      "updated": "2024-07-04T18:33:00+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12354v2",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2402.12354v2.pdf"
    },
    {
      "arxiv_id": "2512.15233v2",
      "title": "Null-LoRA: Low-Rank Adaptation on Null Space",
      "abstract": "Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.",
      "authors": [
        "Yi Zhang",
        "Yulei Kang",
        "Haoxuan Chen",
        "Jinxuan Li",
        "Jian-Fang Hu"
      ],
      "published": "2025-12-17T09:32:19+00:00",
      "updated": "2025-12-18T07:45:14+00:00",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.15233v2",
      "primary_category": "cs.CV",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2512.15233v2.pdf"
    },
    {
      "arxiv_id": "2506.20629v1",
      "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models",
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used finetuning method for large models. Its small memory footprint allows practitioners to adapt large models to specific tasks at a fraction of the cost of full finetuning. Different modifications have been proposed to enhance its efficiency by, for example, setting the learning rate, the rank, and the initialization. Another improvement axis is adapter placement strategy: when using LoRA, practitioners usually pick module types to adapt with LoRA, such as Query and Key modules. Few works have studied the problem of adapter placement, with nonconclusive results: original LoRA paper suggested placing adapters in attention modules, while other works suggested placing them in the MLP modules. Through an intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task. We demonstrate that PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning.",
      "authors": [
        "Soufiane Hayou",
        "Nikhil Ghosh",
        "Bin Yu"
      ],
      "published": "2025-06-25T17:25:02+00:00",
      "updated": "2025-06-25T17:25:02+00:00",
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.20629v1",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2506.20629v1.pdf"
    },
    {
      "arxiv_id": "2412.09827v1",
      "title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models",
      "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low dimensional. Although LoRA has demonstrated commendable performance, there remains a significant performance gap between LoRA and full fine-tuning when learning new tasks. In this work, we propose Low-Rank Adaptation with Task-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features from the perspective of editing neural network representations. To prioritize task-relevant features, a task-aware filter that selectively extracts valuable knowledge from hidden representations for the target or current task is designed. As the experiments on a vareity of datasets including NLU, commonsense reasoning and mathematical reasoning tasks demonstrates, our method reduces 33.71% parameters and achieves better performance on a variety of datasets in comparison with SOTA low-rank methods.",
      "authors": [
        "Changqun Li",
        "Chaofan Ding",
        "Kexin Luan",
        "Xinhan Di"
      ],
      "published": "2024-12-13T03:38:49+00:00",
      "updated": "2024-12-13T03:38:49+00:00",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.09827v1",
      "primary_category": "cs.CL",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2412.09827v1.pdf"
    },
    {
      "arxiv_id": "2407.18242v3",
      "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?",
      "abstract": "Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models. Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning. In this paper, we first uncover a fundamental connection between the optimization processes of LoRA and full fine-tuning: using LoRA for optimization is mathematically equivalent to full fine-tuning using a low-rank gradient for parameter updates. And this low-rank gradient can be expressed in terms of the gradients of the two low-rank matrices in LoRA. Leveraging this insight, we introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of these low-rank matrices. This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning. Furthermore, we theoretically derive the optimal solutions for adjusting the gradients of the low-rank matrices, applying them during fine-tuning in LoRA-Pro. We conduct extensive experiments across natural language understanding, dialogue generation, mathematical reasoning, code generation, and image classification tasks, demonstrating that LoRA-Pro substantially improves LoRA's performance, effectively narrowing the gap with full fine-tuning. Code is publicly available at https://github.com/mrflogs/LoRA-Pro.",
      "authors": [
        "Zhengbo Wang",
        "Jian Liang",
        "Ran He",
        "Zilei Wang",
        "Tieniu Tan"
      ],
      "published": "2024-07-25T17:57:12+00:00",
      "updated": "2025-03-22T09:29:15+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18242v3",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2407.18242v3.pdf"
    },
    {
      "arxiv_id": "1305.5826v1",
      "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations",
      "abstract": "Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.",
      "authors": [
        "Jie Chen",
        "Nannan Cao",
        "Kian Hsiang Low",
        "Ruofei Ouyang",
        "Colin Keng-Yan Tan",
        "Patrick Jaillet"
      ],
      "published": "2013-05-24T19:00:28+00:00",
      "updated": "2013-05-24T19:00:28+00:00",
      "categories": [
        "stat.ML",
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1305.5826v1",
      "primary_category": "stat.ML",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/1305.5826v1.pdf"
    },
    {
      "arxiv_id": "0106135v2",
      "title": "Superstructures at low spin - high spin transitions",
      "abstract": "In many transition metal compounds, in particular those containing Fe^{2+} and Co^{3+}, there occur spin-state transitions between low-spin and high-spin (or intermediate-spin) states. We show that typical interactions between similar spin-state ions are short-range repulsion, and long-range interaction which can have different sign depending on the elastic anisotropy of the lattice and on the direction between respective ions. Due to such character of effective interactions at the spin-state transitions there may occur different superstructures -- ordered arrangement of different spin-states, which in particular may have the form of stripes. The properties of the system TlSr_2CoO_5 for which such a superstructure was recently observed experimentally, are discussed from this point of view.",
      "authors": [
        "D. I. Khomskii",
        "U. Löw"
      ],
      "published": "2001-06-07T16:46:56+00:00",
      "updated": "2003-11-28T13:53:56+00:00",
      "categories": [
        "cond-mat.str-el"
      ],
      "pdf_url": "https://arxiv.org/pdf/cond-mat/0106135v2",
      "primary_category": "cond-mat.str-el",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/0106135v2.pdf"
    },
    {
      "arxiv_id": "2410.20625v2",
      "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
      "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\\% accuracy gain on Super-Natural Instructions and 3.5\\% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
      "authors": [
        "Jui-Nan Yen",
        "Si Si",
        "Zhao Meng",
        "Felix Yu",
        "Sai Surya Duvvuri",
        "Inderjit S. Dhillon",
        "Cho-Jui Hsieh",
        "Sanjiv Kumar"
      ],
      "published": "2024-10-27T22:57:12+00:00",
      "updated": "2025-07-16T21:18:50+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20625v2",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2410.20625v2.pdf"
    },
    {
      "arxiv_id": "2510.09561v2",
      "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control",
      "abstract": "Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.",
      "authors": [
        "Minkyoung Cho",
        "Ruben Ohana",
        "Christian Jacobsen",
        "Adityan Jothi",
        "Min-Hung Chen",
        "Z. Morley Mao",
        "Ethem Can"
      ],
      "published": "2025-10-10T17:13:02+00:00",
      "updated": "2025-12-15T08:57:20+00:00",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.09561v2",
      "primary_category": "cs.CV",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2510.09561v2.pdf"
    },
    {
      "arxiv_id": "2106.09685v2",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
      "authors": [
        "Edward J. Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "published": "2021-06-17T17:37:18+00:00",
      "updated": "2021-10-16T18:40:34+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.09685v2",
      "primary_category": "cs.CL",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2106.09685v2.pdf"
    },
    {
      "arxiv_id": "2406.03136v2",
      "title": "Computational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models",
      "abstract": "We study the computational limits of Low-Rank Adaptation (LoRA) for finetuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of almost linear algorithms by controlling the LoRA update computation term by term. For the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $X$, pretrained weights ${W^\\star}$, and adapter matrices $αB A/r$. Specifically, we derive a shared upper bound threshold for such norms, and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold. For the latter, we prove the existence of almost linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations. To showcase our theory, we consider two practical scenarios: partial (e.g., only $W_V$ and $W_Q$) and full adaptations (e.g., $W_Q$, $W_V$, and $W_K$) of weights in attention heads.",
      "authors": [
        "Jerry Yao-Chieh Hu",
        "Maojiang Su",
        "En-Jui Kuo",
        "Zhao Song",
        "Han Liu"
      ],
      "published": "2024-06-05T10:44:08+00:00",
      "updated": "2025-06-06T05:22:09+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03136v2",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2406.03136v2.pdf"
    },
    {
      "arxiv_id": "2601.21109v1",
      "title": "ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference",
      "abstract": "Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.",
      "authors": [
        "Ketan Thakkar",
        "Maitreyi Chatterjee",
        "Ramasubramanian Balasubramanian",
        "Achyuthan Jootoo",
        "Rajendra Ugrani"
      ],
      "published": "2026-01-28T22:58:28+00:00",
      "updated": "2026-01-28T22:58:28+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.21109v1",
      "primary_category": "cs.CL",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2601.21109v1.pdf"
    },
    {
      "arxiv_id": "2602.05988v1",
      "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
      "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
      "authors": [
        "Keith Ando Ogawa",
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Edson Bollis",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "published": "2026-02-05T18:38:53+00:00",
      "updated": "2026-02-05T18:38:53+00:00",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.05988v1",
      "primary_category": "cs.LG",
      "query_source": "LoRA low rank adaptation",
      "local_pdf_path": "data/raw/2602.05988v1.pdf"
    },
    {
      "arxiv_id": "2510.25518v1",
      "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation",
      "abstract": "Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.",
      "authors": [
        "Thomas Cook",
        "Richard Osuagwu",
        "Liman Tsatiashvili",
        "Vrynsia Vrynsia",
        "Koustav Ghosal",
        "Maraim Masoud",
        "Riccardo Mattivi"
      ],
      "published": "2025-10-29T13:41:36+00:00",
      "updated": "2025-10-29T13:41:36+00:00",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.25518v1",
      "primary_category": "cs.AI",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2510.25518v1.pdf"
    },
    {
      "arxiv_id": "2502.13957v2",
      "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.",
      "authors": [
        "Guangzhi Xiong",
        "Qiao Jin",
        "Xiao Wang",
        "Yin Fang",
        "Haolin Liu",
        "Yifan Yang",
        "Fangyuan Chen",
        "Zhixing Song",
        "Dengyu Wang",
        "Minjia Zhang",
        "Zhiyong Lu",
        "Aidong Zhang"
      ],
      "published": "2025-02-19T18:56:03+00:00",
      "updated": "2025-05-31T23:32:16+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.13957v2",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2502.13957v2.pdf"
    },
    {
      "arxiv_id": "2401.15391v1",
      "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
      "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
      "authors": [
        "Yixuan Tang",
        "Yi Yang"
      ],
      "published": "2024-01-27T11:41:48+00:00",
      "updated": "2024-01-27T11:41:48+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15391v1",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2401.15391v1.pdf"
    },
    {
      "arxiv_id": "2412.12881v1",
      "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement",
      "abstract": "Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.",
      "authors": [
        "Jinhao Jiang",
        "Jiayi Chen",
        "Junyi Li",
        "Ruiyang Ren",
        "Shijie Wang",
        "Wayne Xin Zhao",
        "Yang Song",
        "Tao Zhang"
      ],
      "published": "2024-12-17T13:05:36+00:00",
      "updated": "2024-12-17T13:05:36+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12881v1",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2412.12881v1.pdf"
    },
    {
      "arxiv_id": "2402.07483v2",
      "title": "T-RAG: Lessons from the LLM Trenches",
      "abstract": "Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.",
      "authors": [
        "Masoomali Fatehkia",
        "Ji Kim Lucas",
        "Sanjay Chawla"
      ],
      "published": "2024-02-12T08:45:08+00:00",
      "updated": "2024-06-06T14:42:47+00:00",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07483v2",
      "primary_category": "cs.AI",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2402.07483v2.pdf"
    },
    {
      "arxiv_id": "2504.01346v4",
      "title": "RAG over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating them with an external knowledge base to improve the answer relevance and accuracy. In real-world scenarios, beyond pure text, a substantial amount of knowledge is stored in tables, and user questions often require retrieving answers that are distributed across multiple tables. Retrieving knowledge from a table corpora (i.e., various individual tables) for a question remains nascent, at least, for (i) how to understand intra- and inter-table knowledge effectively, (ii) how to filter unnecessary tables and how to retrieve the most relevant tables efficiently, (iii) how to prompt LLMs to infer over the retrieval, (iv) how to evaluate the corresponding performance in a realistic setting. Facing the above challenges, in this paper, we first propose a table-corpora-aware RAG framework, named T-RAG, which consists of the hierarchical memory index, multi-stage retrieval, and graph-aware prompting for effective and efficient table knowledge retrieval and inference. Further, we first develop a multi-table question answering benchmark named MultiTableQA, which spans 3 different task types, 57,193 tables, and 23,758 questions in total, and the sources are all from real-world scenarios. Based on MultiTableQA, we did the holistic comparison over table retrieval methods, RAG methods, and table-to-graph representation learning methods, where T-RAG shows the leading accuracy, recall, and running time performance. Also, under T-RAG, we evaluate the inference ability upgrade of different LLMs. Code and Data are available at https://github.com/jiaruzouu/T-RAG",
      "authors": [
        "Jiaru Zou",
        "Dongqi Fu",
        "Sirui Chen",
        "Xinrui He",
        "Zihao Li",
        "Yada Zhu",
        "Jiawei Han",
        "Jingrui He"
      ],
      "published": "2025-04-02T04:24:41+00:00",
      "updated": "2025-10-05T07:24:41+00:00",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.01346v4",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2504.01346v4.pdf"
    },
    {
      "arxiv_id": "2507.21753v1",
      "title": "Towards a rigorous evaluation of RAG systems: the challenge of due diligence",
      "abstract": "The rise of generative AI, has driven significant advancements in high-risk sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG) architecture, combining language models (LLMs) with search engines, is particularly notable for its ability to generate responses from document corpora. Despite its potential, the reliability of RAG systems in critical contexts remains a concern, with issues such as hallucinations persisting. This study evaluates a RAG system used in due diligence for an investment fund. We propose a robust evaluation protocol combining human annotations and LLM-Judge annotations to identify system failures, like hallucinations, off-topic, failed citations, and abstentions. Inspired by the Prediction Powered Inference (PPI) method, we achieve precise performance measurements with statistical guarantees. We provide a comprehensive dataset for further analysis. Our contributions aim to enhance the reliability and scalability of RAG systems evaluation protocols in industrial applications.",
      "authors": [
        "Grégoire Martinon",
        "Alexandra Lorenzo de Brionne",
        "Jérôme Bohard",
        "Antoine Lojou",
        "Damien Hervault",
        "Nicolas J-B. Brunel"
      ],
      "published": "2025-07-29T12:33:16+00:00",
      "updated": "2025-07-29T12:33:16+00:00",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.21753v1",
      "primary_category": "cs.AI",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2507.21753v1.pdf"
    },
    {
      "arxiv_id": "2409.03708v2",
      "title": "RAG based Question-Answering for Contextual Response Prediction System",
      "abstract": "Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.",
      "authors": [
        "Sriram Veturi",
        "Saurabh Vaichal",
        "Reshma Lal Jagadheesh",
        "Nafis Irtiza Tripto",
        "Nian Yan"
      ],
      "published": "2024-09-05T17:14:23+00:00",
      "updated": "2024-09-06T14:18:20+00:00",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03708v2",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2409.03708v2.pdf"
    },
    {
      "arxiv_id": "2511.22858v1",
      "title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms",
      "abstract": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.",
      "authors": [
        "Yuya Ishihara",
        "Atsushi Keyaki",
        "Hiroaki Yamada",
        "Ryutaro Ohara",
        "Mihoko Sumida"
      ],
      "published": "2025-11-28T03:28:27+00:00",
      "updated": "2025-11-28T03:28:27+00:00",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.22858v1",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2511.22858v1.pdf"
    },
    {
      "arxiv_id": "2602.20735v1",
      "title": "RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition",
      "abstract": "This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\n  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n  (R2RAG), a research-focused retrieval-augmented generation (RAG)\n  architecture composed of lightweight components that dynamically adapt the\n  retrieval strategy based on inferred query complexity and evidence\n  sufficiency. The system uses smaller LLMs, enabling operation on a single\n  consumer-grade GPU while supporting complex research tasks. It builds on the\n  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\n  with modules informed by qualitative review of outputs. R2RAG won the Best\n  Dynamic Evaluation award in the Open Source category, demonstrating high\n  effectiveness with careful design and efficient use of resources.",
      "authors": [
        "Kun Ran",
        "Marwah Alaofi",
        "Danula Hettiachchi",
        "Chenglong Ma",
        "Khoi Nguyen Dinh Anh",
        "Khoi Vo Nguyen",
        "Sachin Pathiyan Cherumanal",
        "Lida Rashidi",
        "Falk Scholer",
        "Damiano Spina",
        "Shuoqi Sun",
        "Oleg Zendel"
      ],
      "published": "2026-02-24T09:58:25+00:00",
      "updated": "2026-02-24T09:58:25+00:00",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.20735v1",
      "primary_category": "cs.IR",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2602.20735v1.pdf"
    },
    {
      "arxiv_id": "2404.15939v3",
      "title": "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications",
      "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.",
      "authors": [
        "Andrei-Laurentiu Bornea",
        "Fadhel Ayed",
        "Antonio De Domenico",
        "Nicola Piovesan",
        "Ali Maatouk"
      ],
      "published": "2024-04-24T15:58:59+00:00",
      "updated": "2024-08-07T09:36:15+00:00",
      "categories": [
        "cs.IR",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.15939v3",
      "primary_category": "cs.IR",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2404.15939v3.pdf"
    },
    {
      "arxiv_id": "2409.18313v5",
      "title": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation",
      "abstract": "There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhorse of large-scale non-parametric knowledge; however, existing techniques do not directly transfer to the embodied domain, which is multimodal, where data is highly correlated, and perception requires abstraction. To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 250 explanation and navigation queries across kilometer-level environments, highlighting its promise as a general-purpose non-parametric system for embodied agents.",
      "authors": [
        "Quanting Xie",
        "So Yeon Min",
        "Pengliang Ji",
        "Yue Yang",
        "Tianyi Zhang",
        "Kedi Xu",
        "Aarav Bajaj",
        "Ruslan Salakhutdinov",
        "Matthew Johnson-Roberson",
        "Yonatan Bisk"
      ],
      "published": "2024-09-26T21:44:11+00:00",
      "updated": "2025-01-21T02:38:32+00:00",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18313v5",
      "primary_category": "cs.RO",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2409.18313v5.pdf"
    },
    {
      "arxiv_id": "2402.01717v1",
      "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
      "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.",
      "authors": [
        "Jaewoong Kim",
        "Moohong Min"
      ],
      "published": "2024-01-26T08:23:29+00:00",
      "updated": "2024-01-26T08:23:29+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01717v1",
      "primary_category": "cs.CL",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2402.01717v1.pdf"
    },
    {
      "arxiv_id": "2106.11517v1",
      "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering",
      "abstract": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment Generation (RAG) architecture in an end-to-end manner. We highlighted the main engineering challenges that needed to be addressed to achieve this objective. We also compare how end-to-end RAG architecture outperforms the original RAG architecture for the task of question answering. We have open-sourced our implementation in the HuggingFace Transformers library.",
      "authors": [
        "Shamane Siriwardhana",
        "Rivindu Weerasekera",
        "Elliott Wen",
        "Suranga Nanayakkara"
      ],
      "published": "2021-06-22T03:17:59+00:00",
      "updated": "2021-06-22T03:17:59+00:00",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.11517v1",
      "primary_category": "cs.IR",
      "query_source": "RAG evaluation",
      "local_pdf_path": "data/raw/2106.11517v1.pdf"
    },
    {
      "arxiv_id": "1112.2155v1",
      "title": "A Concurrency Control Method Based on Commitment Ordering in Mobile Databases",
      "abstract": "Disconnection of mobile clients from server, in an unclear time and for an unknown duration, due to mobility of mobile clients, is the most important challenges for concurrency control in mobile database with client-server model. Applying pessimistic common classic methods of concurrency control (like 2pl) in mobile database leads to long duration blocking and increasing waiting time of transactions. Because of high rate of aborting transactions, optimistic methods aren`t appropriate in mobile database. In this article, OPCOT concurrency control algorithm is introduced based on optimistic concurrency control method. Reducing communications between mobile client and server, decreasing blocking rate and deadlock of transactions, and increasing concurrency degree are the most important motivation of using optimistic method as the basis method of OPCOT algorithm. To reduce abortion rate of transactions, in execution time of transactions` operators a timestamp is assigned to them. In other to checking commitment ordering property of scheduler, the assigned timestamp is used in server on time of commitment. In this article, serializability of OPCOT algorithm scheduler has been proved by using serializability graph. Results of evaluating simulation show that OPCOT algorithm decreases abortion rate and waiting time of transactions in compare to 2pl and optimistic algorithms.",
      "authors": [
        "Ali Karami",
        "Ahmad Baraani-Dastjerdi"
      ],
      "published": "2011-12-09T17:27:51+00:00",
      "updated": "2011-12-09T17:27:51+00:00",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/1112.2155v1",
      "primary_category": "cs.DB",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1112.2155v1.pdf"
    },
    {
      "arxiv_id": "2107.11458v3",
      "title": "Variant interpretation using population databases: lessons from gnomAD",
      "abstract": "Reference population databases are an essential tool in variant and gene interpretation. Their use guides the identification of pathogenic variants amidst the sea of benign variation present in every human genome, and supports the discovery of new disease-gene relationships. The Genome Aggregation Database (gnomAD) is currently the largest and most widely used publicly available collection of population variation from harmonized sequencing data. The data is available through the online gnomAD browser (https://gnomad.broadinstitute.org/) that enables rapid and intuitive variant analysis. This review provides guidance on the content of the gnomAD browser, and its usage for variant and gene interpretation. We introduce key features including allele frequency, per-base expression levels, constraint scores, and variant co-occurrence, alongside guidance on how to use these in analysis, with a focus on the interpretation of candidate variants and novel genes in rare disease.",
      "authors": [
        "Sanna Gudmundsson",
        "Moriel Singer-Berk",
        "Nicholas A. Watts",
        "William Phu",
        "Julia K. Goodrich",
        "Matthew Solomonson",
        "Genome Aggregation Database Consortium",
        "Heidi L. Rehm",
        "Daniel G. MacArthur",
        "Anne ODonnell-Luria"
      ],
      "published": "2021-07-23T20:56:50+00:00",
      "updated": "2022-01-16T18:36:52+00:00",
      "categories": [
        "q-bio.GN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.11458v3",
      "primary_category": "q-bio.GN",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2107.11458v3.pdf"
    },
    {
      "arxiv_id": "1304.7094v1",
      "title": "A new Watermarking Technique for Secure Database",
      "abstract": "Digital multimedia watermarking technology was suggested in the last decade to embed copyright information in digital objects such images, audio and video. However, the increasing use of relational database systems in many real-life applications created an ever increasing need for watermarking database systems. As a result, watermarking relational database systems is now merging as a research area that deals with the legal issue of copyright protection of database systems. Approach: In this study, we proposed an efficient database watermarking algorithm based on inserting binary image watermarks in non-numeric mutli-word attributes of selected database tuples. Results: The algorithm is robust as it resists attempts to remove or degrade the embedded watermark and it is blind as it does not require the original database in order to extract the embedded watermark. Conclusion: Experimental results demonstrated blindness and the robustness of the algorithm against common database attacks.",
      "authors": [
        "Jun Ziang Pinn",
        "A. Fr. Zung"
      ],
      "published": "2013-04-26T08:46:42+00:00",
      "updated": "2013-04-26T08:46:42+00:00",
      "categories": [
        "cs.DB",
        "cs.CR",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/1304.7094v1",
      "primary_category": "cs.DB",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1304.7094v1.pdf"
    },
    {
      "arxiv_id": "2008.13454v1",
      "title": "Complex-valued embeddings of generic proximity data",
      "abstract": "Proximities are at the heart of almost all machine learning methods. If the input data are given as numerical vectors of equal lengths, euclidean distance, or a Hilbertian inner product is frequently used in modeling algorithms. In a more generic view, objects are compared by a (symmetric) similarity or dissimilarity measure, which may not obey particular mathematical properties. This renders many machine learning methods invalid, leading to convergence problems and the loss of guarantees, like generalization bounds. In many cases, the preferred dissimilarity measure is not metric, like the earth mover distance, or the similarity measure may not be a simple inner product in a Hilbert space but in its generalization a Krein space. If the input data are non-vectorial, like text sequences, proximity-based learning is used or ngram embedding techniques can be applied. Standard embeddings lead to the desired fixed-length vector encoding, but are costly and have substantial limitations in preserving the original data's full information. As an information preserving alternative, we propose a complex-valued vector embedding of proximity data. This allows suitable machine learning algorithms to use these fixed-length, complex-valued vectors for further processing. The complex-valued data can serve as an input to complex-valued machine learning algorithms. In particular, we address supervised learning and use extensions of prototype-based learning. The proposed approach is evaluated on a variety of standard benchmarks and shows strong performance compared to traditional techniques in processing non-metric or non-psd proximity data.",
      "authors": [
        "Maximilian Münch",
        "Michiel Straat",
        "Michael Biehl",
        "Frank-Michael Schleif"
      ],
      "published": "2020-08-31T09:40:30+00:00",
      "updated": "2020-08-31T09:40:30+00:00",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2008.13454v1",
      "primary_category": "cs.LG",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2008.13454v1.pdf"
    },
    {
      "arxiv_id": "2106.07186v1",
      "title": "Sejong Face Database: A Multi-Modal Disguise Face Database",
      "abstract": "Commercial application of facial recognition demands robustness to a variety of challenges such as illumination, occlusion, spoofing, disguise, etc. Disguised face recognition is one of the emerging issues for access control systems, such as security checkpoints at the borders. However, the lack of availability of face databases with a variety of disguise addons limits the development of academic research in the area. In this paper, we present a multimodal disguised face dataset to facilitate the disguised face recognition research. The presented database contains 8 facial add-ons and 7 additional combinations of these add-ons to create a variety of disguised face images. Each facial image is captured in visible, visible plus infrared, infrared, and thermal spectra. Specifically, the database contains 100 subjects divided into subset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus images per modality). We also present baseline face detection results performed on the proposed database to provide reference results and compare the performance in different modalities. Qualitative and quantitative analysis is performed to evaluate the challenging nature of disguise addons. The dataset will be publicly available with the acceptance of the research article. The database is available at: https://github.com/usmancheema89/SejongFaceDatabase.",
      "authors": [
        "Usman Cheema",
        "Seungbin Moon"
      ],
      "published": "2021-06-14T06:29:41+00:00",
      "updated": "2021-06-14T06:29:41+00:00",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.07186v1",
      "primary_category": "cs.CV",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2106.07186v1.pdf"
    },
    {
      "arxiv_id": "1311.7295v1",
      "title": "Glasgow's Stereo Image Database of Garments",
      "abstract": "To provide insight into cloth perception and manipulation with an active binocular robotic vision system, we compiled a database of 80 stereo-pair colour images with corresponding horizontal and vertical disparity maps and mask annotations, for 3D garment point cloud rendering has been created and released. The stereo-image garment database is part of research conducted under the EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs to a wider database collection released through CloPeMa (www.clopema.eu). This database is based on 16 different off-the-shelve garments. Each garment has been imaged in five different pose configurations on the project's binocular robot head. A full copy of the database is made available for scientific research only at https://sites.google.com/site/ugstereodatabase/.",
      "authors": [
        "Gerardo Aragon-Camarasa",
        "Susanne B. Oehler",
        "Yuan Liu",
        "Sun Li",
        "Paul Cockshott",
        "J. Paul Siebert"
      ],
      "published": "2013-11-28T12:09:28+00:00",
      "updated": "2013-11-28T12:09:28+00:00",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1311.7295v1",
      "primary_category": "cs.RO",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1311.7295v1.pdf"
    },
    {
      "arxiv_id": "2505.12524v1",
      "title": "HAKES: Scalable Vector Database for Embedding Search Service",
      "abstract": "Modern deep learning models capture the semantics of complex data by transforming them into high-dimensional embedding vectors. Emerging applications, such as retrieval-augmented generation, use approximate nearest neighbor (ANN) search in the embedding vector space to find similar data. Existing vector databases provide indexes for efficient ANN searches, with graph-based indexes being the most popular due to their low latency and high recall in real-world high-dimensional datasets. However, these indexes are costly to build, suffer from significant contention under concurrent read-write workloads, and scale poorly to multiple servers.\n  Our goal is to build a vector database that achieves high throughput and high recall under concurrent read-write workloads. To this end, we first propose an ANN index with an explicit two-stage design combining a fast filter stage with highly compressed vectors and a refine stage to ensure recall, and we devise a novel lightweight machine learning technique to fine-tune the index parameters. We introduce an early termination check to dynamically adapt the search process for each query. Next, we add support for writes while maintaining search performance by decoupling the management of the learned parameters. Finally, we design HAKES, a distributed vector database that serves the new index in a disaggregated architecture. We evaluate our index and system against 12 state-of-the-art indexes and three distributed vector databases, using high-dimensional embedding datasets generated by deep learning models. The experimental results show that our index outperforms index baselines in the high recall region and under concurrent read-write workloads. Furthermore, \\namesys{} is scalable and achieves up to $16\\times$ higher throughputs than the baselines. The HAKES project is open-sourced at https://www.comp.nus.edu.sg/~dbsystem/hakes/.",
      "authors": [
        "Guoyu Hu",
        "Shaofeng Cai",
        "Tien Tuan Anh Dinh",
        "Zhongle Xie",
        "Cong Yue",
        "Gang Chen",
        "Beng Chin Ooi"
      ],
      "published": "2025-05-18T19:26:29+00:00",
      "updated": "2025-05-18T19:26:29+00:00",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12524v1",
      "primary_category": "cs.DB",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2505.12524v1.pdf"
    },
    {
      "arxiv_id": "2306.15295v2",
      "title": "Synthesis of Quantum Vector Databases Based on Grovers Algorithm",
      "abstract": "This paper describes a method for using Grovers algorithm to create a quantum vector database, the database stores embeddings based on Controlled-S gates, which represent a binary numerical value. This value represents the embeddings value. The process of creating meaningful embeddings is handled by a classical computer and the search process is handled by the quantum computer. This search approach might be beneficial for a large enough database, or it could be seen as a very qubit-efficient (super dense) way for storing data on a quantum computer, since the proposed circuit stores many embeddings inside one quantum register simultaneously.",
      "authors": [
        "Cesar Borisovich Pronin",
        "Andrey Vladimirovich Ostroukh"
      ],
      "published": "2023-06-27T08:34:15+00:00",
      "updated": "2023-10-04T21:04:51+00:00",
      "categories": [
        "quant-ph",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15295v2",
      "primary_category": "quant-ph",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2306.15295v2.pdf"
    },
    {
      "arxiv_id": "2505.06386v2",
      "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization",
      "abstract": "Embedding projections are popular for visualizing large datasets and models. However, people often encounter \"friction\" when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms -- including density-based clustering, and automated labeling -- to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis.",
      "authors": [
        "Donghao Ren",
        "Fred Hohman",
        "Halden Lin",
        "Dominik Moritz"
      ],
      "published": "2025-05-09T19:15:54+00:00",
      "updated": "2025-07-08T17:49:59+00:00",
      "categories": [
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06386v2",
      "primary_category": "cs.HC",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2505.06386v2.pdf"
    },
    {
      "arxiv_id": "2502.09035v1",
      "title": "Implementation of a Fuzzy Relational Database. Case Study: Chilean Cardboard Industry in the Maule Region",
      "abstract": "The international database community refers to the manipulation of data with inaccuracy and uncertainty using the term fuzzy, which has been translated into Spanish as \"borroso\" and into French as \"flou\". Semantically, this term conveys two main ideas: first, the natural concept of ambiguity or vagueness in human reasoning, and second, its connection to fuzzy set theory, fuzzy logic, and possibility theory, as developed by Zadeh between 1965 and 1977. This article explores two key aspects: the attributes of the fuzzy data model GEFRED (GENeralized model for Fuzzy RElational Database) and their implementation in a Relational Database (RDB). The modeling of these attributes was conducted in a Chilian cardboard manufacturing company located in the Maule Region, where the described phenomena involve imprecise and uncertain attributes and values. Specifically, our focus is on the knowledge related to the manufacturing process of coated cardboard, particularly the quality control process for finished products in the company's Conversion Department. The quality of these products, categorized as either stacks or rolls, is characterized using both classical and fuzzy attributes. Classical attributes are typically measured with physical instruments, whereas fuzzy attributes are assessed through human senses, primarily sight and touch, as perceived by the operators.",
      "authors": [
        "Leoncio Jimenez",
        "Angélica Urrutia",
        "José Galindo",
        "Pascale Zaraté"
      ],
      "published": "2025-02-13T07:42:37+00:00",
      "updated": "2025-02-13T07:42:37+00:00",
      "categories": [
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09035v1",
      "primary_category": "cs.CY",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2502.09035v1.pdf"
    },
    {
      "arxiv_id": "1411.2596v1",
      "title": "The AGN Black Hole Mass Database",
      "abstract": "The AGN Black Hole Mass Database is a compilation of all published spectroscopic reverberation-mapping studies of active galaxies. We have created a public web interface, where users may get the most up-to-date black hole masses from reverberation mapping for any particular active galactic nucleus (AGN), as well as obtain the individual measurements upon which the masses are based and the appropriate references. While the database currently focuses on the measurements necessary for black hole mass determinations, we also plan to expand it in the future to include additional useful information, such as host-galaxy characteristics. New reverberation mapping results will also be incorporated into the database as they are published in peer-refereed journals.",
      "authors": [
        "Misty C. Bentz",
        "Sarah Katz"
      ],
      "published": "2014-11-10T21:00:09+00:00",
      "updated": "2014-11-10T21:00:09+00:00",
      "categories": [
        "astro-ph.GA"
      ],
      "pdf_url": "https://arxiv.org/pdf/1411.2596v1",
      "primary_category": "astro-ph.GA",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1411.2596v1.pdf"
    },
    {
      "arxiv_id": "9906195v1",
      "title": "Observational Mishaps - a Database",
      "abstract": "We present a World-Wide-Web-accessible database of astronomical images which suffer from a variety of observational problems, ranging from common occurences, such as dust grains on filters and/or the dewar window, to more exotic phenomena, such as loss of primary mirror support due to the deflation of the support airbags. Apart from its educational usefulness, the purpose of this database is to assist astronomers in diagnosing and treating errant images at the telescope, thus saving valuable telescope time. Every observational mishap contained in this on-line catalog is presented in the form of a GIF image, a brief explanation of the problem, and, when possible, a suggestion for improving the image quality.",
      "authors": [
        "Kaspar von Braun",
        "Kristin Chiboucas",
        "Denise Hurley-Keller"
      ],
      "published": "1999-06-11T00:39:03+00:00",
      "updated": "1999-06-11T00:39:03+00:00",
      "categories": [
        "astro-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/astro-ph/9906195v1",
      "primary_category": "astro-ph",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/9906195v1.pdf"
    },
    {
      "arxiv_id": "1708.04557v1",
      "title": "Database of Parliamentary Speeches in Ireland, 1919-2013",
      "abstract": "We present a database of parliamentary debates that contains the complete record of parliamentary speeches from Dáil Éireann, the lower house and principal chamber of the Irish parliament, from 1919 to 2013. In addition, the database contains background information on all TDs (Teachta Dála, members of parliament), such as their party affiliations, constituencies and office positions. The current version of the database includes close to 4.5 million speeches from 1,178 TDs. The speeches were downloaded from the official parliament website and further processed and parsed with a Python script. Background information on TDs was collected from the member database of the parliament website. Data on cabinet positions (ministers and junior ministers) was collected from the official website of the government. A record linkage algorithm and human coders were used to match TDs and ministers.",
      "authors": [
        "Alexander Herzog",
        "Slava J. Mikhaylov"
      ],
      "published": "2017-08-15T15:34:33+00:00",
      "updated": "2017-08-15T15:34:33+00:00",
      "categories": [
        "cs.CL",
        "cs.SI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1708.04557v1",
      "primary_category": "cs.CL",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1708.04557v1.pdf"
    },
    {
      "arxiv_id": "1808.09571v1",
      "title": "Full Speed Ahead: 3D Spatial Database Acceleration with GPUs",
      "abstract": "Many industries rely on visual insights to support decision- making processes in their businesses. In mining, the analysis of drills and geological shapes, represented as 3D geometries, is an important tool to assist geologists on the search for new ore deposits. Aeronautics manipulate high-resolution geometries when designing a new aircraft aided by the numerical simulation of aerodynamics. In common, these industries require scalable databases that compute spatial relationships and measurements so that decision making can be conducted without lags. However, as we show in this study, most database systems either lack support for handling 3D geometries or show poor performance when given a sheer volume of data to work with. This paper presents a pluggable acceleration engine for spatial database systems that can improve the performance of spatial operations by more than 3000x through GPU offloading. We focus on the design and evaluation of our plug-in for the PostgreSQL database system.",
      "authors": [
        "Lucas C. Villa Real",
        "Bruno Silva"
      ],
      "published": "2018-08-28T22:55:44+00:00",
      "updated": "2018-08-28T22:55:44+00:00",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/1808.09571v1",
      "primary_category": "cs.DC",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/1808.09571v1.pdf"
    },
    {
      "arxiv_id": "2506.01617v1",
      "title": "An Open and Collaborative Database of Properties of Materials for High-Temperature Superconducting-Based Devices",
      "abstract": "The successful integration of high-temperature superconductors (HTS) into modern technologies requires consistent, accessible, and comprehensive material data, a need that is currently unmet due to the fragmented and incomplete nature of existing resources. This paper introduces a new collaborative, open-access database specifically designed to address this gap by providing standardized data on HTS materials and crucial auxiliary components for HTS applications. The database encompasses extensive data on structural, cryogenic, electrical, magnetic, and superconducting materials, supporting diverse requirements from HTS modelling to magnet design. Developed through collaborative efforts and organized using an ontology-driven data model, this platform is dynamically adaptable, ensuring that it can grow as new materials and data emerge. Key features include user-driven contributions, peer-reviewed data validation, and advanced filtering capabilities for efficient data retrieval. This innovative database, to the knowledge of the authors, being the largest publicly available for material properties of HTS technologies is positioned as a valuable tool for the HTS community, promoting more efficient research and development processes, accelerating the practical application of HTS, and fostering a collaborative approach to knowledge sharing within the field. The database is available at https://sc.hi-scale.grisenergia.pt/app.",
      "authors": [
        "Pablo Cayado",
        "João Rosas",
        "João Murta-Pina",
        "Harold S. Ruiz"
      ],
      "published": "2025-06-02T12:56:51+00:00",
      "updated": "2025-06-02T12:56:51+00:00",
      "categories": [
        "cond-mat.supr-con",
        "physics.app-ph",
        "physics.data-an"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01617v1",
      "primary_category": "cond-mat.supr-con",
      "query_source": "vector database embedding",
      "local_pdf_path": "data/raw/2506.01617v1.pdf"
    },
    {
      "arxiv_id": "0512170v1",
      "title": "Active Amplification of the Terrestrial Albedo to Mitigate Climate Change: An Exploratory Study",
      "abstract": "This study explores the potential to enhance the reflectance of solar insolation by the human settlement and grassland components of the Earth's terrestrial surface as a climate change mitigation measure. Preliminary estimates derived using a static radiative transfer model indicate that such efforts could amplify the planetary albedo enough to offset the current global annual average level of radiative forcing caused by anthropogenic greenhouse gases by as much as 30 percent or 0.76 W/m2. Terrestrial albedo amplification may thus extend, by about 25 years, the time available to advance the development and use of low-emission energy conversion technologies which ultimately remain essential to mitigate long-term climate change. However, additional study is needed to confirm the estimates reported here and to assess the economic and environmental impacts of active land-surface albedo amplification as a climate change mitigation measure.",
      "authors": [
        "Robert M. Hamwey"
      ],
      "published": "2005-12-19T19:08:21+00:00",
      "updated": "2005-12-19T19:08:21+00:00",
      "categories": [
        "physics.ao-ph",
        "physics.geo-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/physics/0512170v1",
      "primary_category": "physics.ao-ph",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/0512170v1.pdf"
    },
    {
      "arxiv_id": "2509.03518v1",
      "title": "Can LLMs Lie? Investigation beyond Hallucination",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. In this work, we systematically investigate the lying behavior of LLMs, differentiating it from hallucinations and testing it in practical scenarios. Through mechanistic interpretability techniques, we uncover the neural mechanisms underlying deception, employing logit lens analysis, causal interventions, and contrastive activation steering to identify and control deceptive behavior. We study real-world lying scenarios and introduce behavioral steering vectors that enable fine-grained manipulation of lying tendencies. Further, we explore the trade-offs between lying and end-task performance, establishing a Pareto frontier where dishonesty can enhance goal optimization. Our findings contribute to the broader discourse on AI ethics, shedding light on the risks and potential safeguards for deploying LLMs in high-stakes environments. Code and more illustrations are available at https://llm-liar.github.io/",
      "authors": [
        "Haoran Huan",
        "Mihir Prabhudesai",
        "Mengning Wu",
        "Shantanu Jaiswal",
        "Deepak Pathak"
      ],
      "published": "2025-09-03T17:59:45+00:00",
      "updated": "2025-09-03T17:59:45+00:00",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.03518v1",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2509.03518v1.pdf"
    },
    {
      "arxiv_id": "2506.00448v1",
      "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization",
      "abstract": "Hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues pose significant risks to patient care and clinical decision-making. However, the phenomenon remains understudied in the clinical domain, with uncertainty surrounding the applicability of general-domain hallucination detectors. The rarity and randomness of hallucinations further complicate their investigation. In this paper, we conduct an evaluation of hallucination detection methods in the medical domain, and construct two datasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by systematically removing facts from source dialogues to induce hallucinated content in summaries; and a natural hallucination dataset -- arising organically during LLM-based medical summarization. We show that general-domain detectors struggle to detect clinical hallucinations, and that performance on fact-controlled hallucinations does not reliably predict effectiveness on natural hallucinations. We then develop fact-based approaches that count hallucinations, offering explainability not available with existing methods. Notably, our LLM-based detectors, which we developed using fact-controlled hallucinations, generalize well to detecting real-world clinical hallucinations. This research contributes a suite of specialized metrics supported by expert-annotated datasets to advance faithful clinical summarization systems.",
      "authors": [
        "Suhas BN",
        "Han-Chin Shing",
        "Lei Xu",
        "Mitch Strong",
        "Jon Burnsky",
        "Jessica Ofor",
        "Jordan R. Mason",
        "Susan Chen",
        "Sundararajan Srinivasan",
        "Chaitanya Shivade",
        "Jack Moriarty",
        "Joseph Paul Cohen"
      ],
      "published": "2025-05-31T08:04:37+00:00",
      "updated": "2025-05-31T08:04:37+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00448v1",
      "primary_category": "cs.CL",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2506.00448v1.pdf"
    },
    {
      "arxiv_id": "2510.19507v2",
      "title": "Teaming LLMs to Detect and Mitigate Hallucinations",
      "abstract": "Recent work has demonstrated state-of-the-art results in large language model (LLM) hallucination detection and mitigation through consistency-based approaches which involve aggregating multiple responses sampled from a single LLM for a given prompt. These approaches help offset limitations stemming from the imperfect data on which LLMs are trained, which includes biases and under-representation of information required at deployment time among other limitations which can lead to hallucinations. We show that extending these single-model consistency methods to combine responses from multiple LLMs with different training data, training schemes and model architectures can result in substantial further improvements in hallucination detection and mitigation capabilities beyond their single-model consistency counterparts. We evaluate this \"consortium consistency\" approach across many model teams from a pool of 15 LLMs and explore under what conditions it is beneficial to team together different LLMs in this manner. Further, we show that these performance improvements often come with reduced inference costs, offsetting a significant drawback with single-model consistency methods.",
      "authors": [
        "Demian Till",
        "John Smeaton",
        "Peter Haubrick",
        "Gouse Saheb",
        "Florian Graef",
        "David Berman"
      ],
      "published": "2025-10-22T12:03:43+00:00",
      "updated": "2025-10-23T15:12:10+00:00",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.19507v2",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2510.19507v2.pdf"
    },
    {
      "arxiv_id": "2502.11306v1",
      "title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation",
      "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.",
      "authors": [
        "Hieu Nguyen",
        "Zihao He",
        "Shoumik Atul Gandre",
        "Ujjwal Pasupulety",
        "Sharanya Kumari Shivakumar",
        "Kristina Lerman"
      ],
      "published": "2025-02-16T23:05:36+00:00",
      "updated": "2025-02-16T23:05:36+00:00",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11306v1",
      "primary_category": "cs.CL",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2502.11306v1.pdf"
    },
    {
      "arxiv_id": "2509.21473v1",
      "title": "Are Hallucinations Bad Estimations?",
      "abstract": "We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.",
      "authors": [
        "Hude Liu",
        "Jerry Yao-Chieh Hu",
        "Jennifer Yuntong Zhang",
        "Zhao Song",
        "Han Liu"
      ],
      "published": "2025-09-25T19:39:09+00:00",
      "updated": "2025-09-25T19:39:09+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.21473v1",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2509.21473v1.pdf"
    },
    {
      "arxiv_id": "2507.20836v4",
      "title": "First Hallucination Tokens Are Different from Conditional Ones",
      "abstract": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to ensuring trust. While many approaches address hallucination detection at the response or span level, recent work explores token-level detection, enabling more fine-grained intervention. However, the distribution of hallucination signal across sequences of hallucinated tokens remains unexplored. We leverage token-level annotations from the RAGTruth corpus and find that the first hallucinated token is far more detectable than later ones. This structural property holds across models, suggesting that first hallucination tokens play a key role in token-level hallucination detection. Our code is available at https://github.com/jakobsnl/RAGTruth_Xtended.",
      "authors": [
        "Jakob Snel",
        "Seong Joon Oh"
      ],
      "published": "2025-07-28T13:44:21+00:00",
      "updated": "2025-10-06T14:29:58+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.20836v4",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2507.20836v4.pdf"
    },
    {
      "arxiv_id": "2409.20550v2",
      "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation",
      "abstract": "Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs. The replication package including code, data, and experimental results is available at https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
      "authors": [
        "Ziyao Zhang",
        "Yanlin Wang",
        "Chong Wang",
        "Jiachi Chen",
        "Zibin Zheng"
      ],
      "published": "2024-09-30T17:51:15+00:00",
      "updated": "2025-01-17T01:44:44+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20550v2",
      "primary_category": "cs.SE",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2409.20550v2.pdf"
    },
    {
      "arxiv_id": "1906.07008v1",
      "title": "Hallucinated Adversarial Learning for Robust Visual Tracking",
      "abstract": "Humans can easily learn new concepts from just a single exemplar, mainly due to their remarkable ability to imagine or hallucinate what the unseen exemplar may look like in different settings. Incorporating such an ability to hallucinate diverse new samples of the tracked instance can help the trackers alleviate the over-fitting problem in the low-data tracking regime. To achieve this, we propose an effective adversarial approach, denoted as adversarial \"hallucinator\" (AH), for robust visual tracking. The proposed AH is designed to firstly learn transferable non-linear deformations between a pair of same-identity instances, and then apply these deformations to an unseen tracked instance in order to generate diverse positive training samples. By incorporating AH into an online tracking-by-detection framework, we propose the hallucinated adversarial tracker (HAT), which jointly optimizes AH with an online classifier (e.g., MDNet) in an end-to-end manner. In addition, a novel selective deformation transfer (SDT) method is presented to better select the deformations which are more suitable for transfer. Extensive experiments on 3 popular benchmarks demonstrate that our HAT achieves the state-of-the-art performance.",
      "authors": [
        "Qiangqiang Wu",
        "Zhihui Chen",
        "Lin Cheng",
        "Yan Yan",
        "Bo Li",
        "Hanzi Wang"
      ],
      "published": "2019-06-17T13:02:23+00:00",
      "updated": "2019-06-17T13:02:23+00:00",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.07008v1",
      "primary_category": "cs.CV",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/1906.07008v1.pdf"
    },
    {
      "arxiv_id": "2507.01446v1",
      "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations",
      "abstract": "Improving customer service quality and response time are critical factors for maintaining customer loyalty and increasing a company's market share. While adopting emerging technologies such as Large Language Models (LLMs) is becoming a necessity to achieve these goals, the risk of hallucination remains a major challenge. In this paper, we present a multi-agent system to handle customer requests sent via SMS. This system integrates LLM based agents with fuzzy logic to mitigate hallucination risks.",
      "authors": [
        "Abd Elrahman Amer",
        "Magdi Amer"
      ],
      "published": "2025-07-02T08:06:02+00:00",
      "updated": "2025-07-02T08:06:02+00:00",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.01446v1",
      "primary_category": "cs.AI",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2507.01446v1.pdf"
    },
    {
      "arxiv_id": "2508.14314v2",
      "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages fine-grained cross-model consistency to detect and mitigate hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves up to 9 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation on multiple datasets demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.",
      "authors": [
        "Aman Goel",
        "Daniel Schwartz",
        "Yanjun Qi"
      ],
      "published": "2025-08-19T23:45:34+00:00",
      "updated": "2025-11-01T18:07:12+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.14314v2",
      "primary_category": "cs.CL",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2508.14314v2.pdf"
    },
    {
      "arxiv_id": "2311.08117v1",
      "title": "Insights into Classifying and Mitigating LLMs' Hallucinations",
      "abstract": "The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as \"Hallucinations\". They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular, Hallucination classification is tackled over several tasks (Machine Translation, Question and Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and Visual Question Answer). Additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project, generously supported by NGI Search, dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.",
      "authors": [
        "Alessandro Bruno",
        "Pier Luigi Mazzeo",
        "Aladine Chetouani",
        "Marouane Tliba",
        "Mohamed Amine Kerkouri"
      ],
      "published": "2023-11-14T12:30:28+00:00",
      "updated": "2023-11-14T12:30:28+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08117v1",
      "primary_category": "cs.CL",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2311.08117v1.pdf"
    },
    {
      "arxiv_id": "2512.03107v1",
      "title": "Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%",
      "abstract": "Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",
      "authors": [
        "Mainak Singha"
      ],
      "published": "2025-12-02T05:25:48+00:00",
      "updated": "2025-12-02T05:25:48+00:00",
      "categories": [
        "cs.LG",
        "cs.CL",
        "q-fin.CP",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.03107v1",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2512.03107v1.pdf"
    },
    {
      "arxiv_id": "2402.10612v3",
      "title": "Rowen: Adaptive Retrieval-Augmented Generation for Hallucination Mitigation in LLMs",
      "abstract": "Hallucinations present a significant challenge for large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. To balance the use of parametric knowledge within LLMs and external information, in this study, we present Rowen, a novel framework that enhances LLMs with an adaptive retrieval augmentation process tailored to address hallucinated outputs. Rowen introduces a consistency-based hallucination detection module, which assesses the model's uncertainty regarding the input query by evaluating the semantic inconsistencies in various responses generated across different languages or models. When high uncertainties in the responses are detected, Rowen activates the retrieval of external information to rectify the model outputs. Through comprehensive empirical experiments, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.",
      "authors": [
        "Hanxing Ding",
        "Liang Pang",
        "Zihao Wei",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "published": "2024-02-16T11:55:40+00:00",
      "updated": "2025-10-04T14:31:52+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.10612v3",
      "primary_category": "cs.CL",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2402.10612v3.pdf"
    },
    {
      "arxiv_id": "2507.15903v1",
      "title": "Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor",
      "abstract": "Empowered by large language models (LLMs), intelligent agents have become a popular paradigm for interacting with open environments to facilitate AI deployment. However, hallucinations generated by LLMs-where outputs are inconsistent with facts-pose a significant challenge, undermining the credibility of intelligent agents. Only if hallucinations can be mitigated, the intelligent agents can be used in real-world without any catastrophic risk. Therefore, effective detection and mitigation of hallucinations are crucial to ensure the dependability of agents. Unfortunately, the related approaches either depend on white-box access to LLMs or fail to accurately identify hallucinations. To address the challenge posed by hallucinations of intelligent agents, we present HalMit, a novel black-box watchdog framework that models the generalization bound of LLM-empowered agents and thus detect hallucinations without requiring internal knowledge of the LLM's architecture. Specifically, a probabilistic fractal sampling technique is proposed to generate a sufficient number of queries to trigger the incredible responses in parallel, efficiently identifying the generalization bound of the target agent. Experimental evaluations demonstrate that HalMit significantly outperforms existing approaches in hallucination monitoring. Its black-box nature and superior performance make HalMit a promising solution for enhancing the dependability of LLM-powered systems.",
      "authors": [
        "Siyuan Liu",
        "Wenjing Liu",
        "Zhiwei Xu",
        "Xin Wang",
        "Bo Chen",
        "Tao Li"
      ],
      "published": "2025-07-21T09:08:58+00:00",
      "updated": "2025-07-21T09:08:58+00:00",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.15903v1",
      "primary_category": "cs.LG",
      "query_source": "LLM hallucination mitigation",
      "local_pdf_path": "data/raw/2507.15903v1.pdf"
    },
    {
      "arxiv_id": "2402.11690v1",
      "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
      "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.",
      "authors": [
        "Zhiyang Xu",
        "Chao Feng",
        "Rulin Shao",
        "Trevor Ashby",
        "Ying Shen",
        "Di Jin",
        "Yu Cheng",
        "Qifan Wang",
        "Lifu Huang"
      ],
      "published": "2024-02-18T19:38:44+00:00",
      "updated": "2024-02-18T19:38:44+00:00",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.11690v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2402.11690v1.pdf"
    },
    {
      "arxiv_id": "2310.04793v2",
      "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
      "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
      "authors": [
        "Neng Wang",
        "Hongyang Yang",
        "Christina Dan Wang"
      ],
      "published": "2023-10-07T12:52:58+00:00",
      "updated": "2023-11-11T06:51:24+00:00",
      "categories": [
        "cs.CL",
        "q-fin.TR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04793v2",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2310.04793v2.pdf"
    },
    {
      "arxiv_id": "2304.03277v1",
      "title": "Instruction Tuning with GPT-4",
      "abstract": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "published": "2023-04-06T17:58:09+00:00",
      "updated": "2023-04-06T17:58:09+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03277v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2304.03277v1.pdf"
    },
    {
      "arxiv_id": "2304.07995v1",
      "title": "From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning",
      "abstract": "Fine-tuning language models on tasks with instructions has demonstrated potential in facilitating zero-shot generalization to unseen tasks. In this paper, we introduce a straightforward yet effective method for enhancing instruction tuning by employing symbolic tasks. Compared to crowdsourced human tasks or model-generated tasks, symbolic tasks present a unique advantage as they can be easily generated in vast quantities, theoretically providing an infinite supply of high-quality training instances. To explore the potential of symbolic tasks, we carry out an extensive case study on the representative symbolic task of SQL execution. Empirical results on various benchmarks validate that the integration of SQL execution leads to significant improvements in zero-shot scenarios, particularly in table reasoning. Notably, our 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table reasoning across four benchmarks. Furthermore, experimental results on BBH (27 tasks) and MMLU (57 tasks) reveal that language models can be enhanced through symbolic tasks without compromising their generality. We hope that our paper serves as a catalyst, inspiring increased efforts to incorporate symbolic tasks in instruction tuning.",
      "authors": [
        "Qian Liu",
        "Fan Zhou",
        "Zhengbao Jiang",
        "Longxu Dou",
        "Min Lin"
      ],
      "published": "2023-04-17T05:29:42+00:00",
      "updated": "2023-04-17T05:29:42+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.07995v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2304.07995v1.pdf"
    },
    {
      "arxiv_id": "2311.13133v1",
      "title": "LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms",
      "abstract": "Large Language Models are traditionally finetuned on large instruction datasets. However recent studies suggest that small, high-quality datasets can suffice for general purpose instruction following. This lack of consensus surrounding finetuning best practices is in part due to rapidly diverging approaches to LLM evaluation. In this study, we ask whether a small amount of diverse finetuning samples can improve performance on both traditional perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We finetune open-source MPT-7B and MPT-30B models on instruction finetuning datasets of various sizes ranging from 1k to 60k samples. We find that subsets of 1k-6k instruction finetuning samples are sufficient to achieve good performance on both (1) traditional NLP benchmarks and (2) model-based evaluation. Finally, we show that mixing textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms.",
      "authors": [
        "Aditi Jha",
        "Sam Havens",
        "Jeremy Dohmann",
        "Alex Trott",
        "Jacob Portes"
      ],
      "published": "2023-11-22T03:37:01+00:00",
      "updated": "2023-11-22T03:37:01+00:00",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.13133v1",
      "primary_category": "cs.LG",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2311.13133v1.pdf"
    },
    {
      "arxiv_id": "2311.18215v1",
      "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
      "abstract": "Caution: this paper may include material that could be offensive or distressing.\n  The advent of Large Language Models (LLMs) necessitates the development of training approaches that mitigate the generation of unethical language and aptly manage toxic user queries. Given the challenges related to human labor and the scarcity of data, we present KoTox, comprising 39K unethical instruction-output pairs. This collection of automatically generated toxic instructions refines the training of LLMs and establishes a foundational framework for improving LLMs' ethical awareness and response to various toxic inputs, promoting more secure and responsible interactions in Natural Language Processing (NLP) applications.",
      "authors": [
        "Sungjoo Byun",
        "Dongjun Jang",
        "Hyemi Jo",
        "Hyopil Shin"
      ],
      "published": "2023-11-30T03:19:45+00:00",
      "updated": "2023-11-30T03:19:45+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.18215v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2311.18215v1.pdf"
    },
    {
      "arxiv_id": "2311.16338v1",
      "title": "Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models",
      "abstract": "Instruction-following language models demand robust methodologies for information retrieval to augment instructions for question-answering applications. A primary challenge is the resolution of coreferences in the context of chunking strategies for long documents. The critical barrier to experimentation of handling coreferences is a lack of open source datasets, specifically in question-answering tasks that require coreference resolution. In this work we present our Coreference Resolution in Question-Answering (CRaQAn) dataset, an open-source dataset that caters to the nuanced information retrieval requirements of coreference resolution in question-answering tasks by providing over 250 question-answer pairs containing coreferences. To develop this dataset, we developed a novel approach for creating high-quality datasets using an instruction-following model (GPT-4) and a Recursive Criticism and Improvement Loop.",
      "authors": [
        "Rob Grzywinski",
        "Joshua D'Arcy",
        "Rob Naidoff",
        "Ashish Shukla",
        "Alex Browne",
        "Ren Gibbons",
        "Brinnae Bent"
      ],
      "published": "2023-11-27T21:54:50+00:00",
      "updated": "2023-11-27T21:54:50+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16338v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2311.16338v1.pdf"
    },
    {
      "arxiv_id": "2405.19744v1",
      "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions",
      "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning.",
      "authors": [
        "Chong Li",
        "Wen Yang",
        "Jiajun Zhang",
        "Jinliang Lu",
        "Shaonan Wang",
        "Chengqing Zong"
      ],
      "published": "2024-05-30T06:45:23+00:00",
      "updated": "2024-05-30T06:45:23+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19744v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2405.19744v1.pdf"
    },
    {
      "arxiv_id": "2505.12716v3",
      "title": "Shadow-FT: Tuning Instruct Model via Training on Paired Base Model",
      "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the Instruct (i.e., instruction-tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired Base models, the foundation for these Instruct variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). The Base model tends to be a good learner yet a weak backbone without post-training. Therefore, we propose a novel Shadow-FT framework to tune the Instruct models by leveraging the corresponding Base models. The key insight is to fine-tune the Base model, and then \\textit{directly} graft the learned weight updates to the Instruct model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization~(DPO). Codes and weights are available at \\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.",
      "authors": [
        "Taiqiang Wu",
        "Runming Yang",
        "Jiayi Li",
        "Pengfei Hu",
        "Yik-Chung Wu",
        "Ngai Wong",
        "Yujiu Yang"
      ],
      "published": "2025-05-19T05:16:21+00:00",
      "updated": "2025-09-26T03:43:47+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12716v3",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2505.12716v3.pdf"
    },
    {
      "arxiv_id": "2408.16440v2",
      "title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain",
      "abstract": "Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.",
      "authors": [
        "Miguel Rios"
      ],
      "published": "2024-08-29T11:05:54+00:00",
      "updated": "2025-07-30T13:06:51+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16440v2",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2408.16440v2.pdf"
    },
    {
      "arxiv_id": "2308.12097v1",
      "title": "Instruction Position Matters in Sequence Generation with Large Language Models",
      "abstract": "Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks.",
      "authors": [
        "Yijin Liu",
        "Xianfeng Zeng",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "published": "2023-08-23T12:36:57+00:00",
      "updated": "2023-08-23T12:36:57+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.12097v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2308.12097v1.pdf"
    },
    {
      "arxiv_id": "2409.14254v1",
      "title": "Instruction Following without Instruction Tuning",
      "abstract": "Instruction tuning commonly means finetuning a language model on instruction-response pairs. We discover two forms of adaptation (tuning) that are deficient compared to instruction tuning, yet still yield instruction following; we call this implicit instruction tuning. We first find that instruction-response pairs are not necessary: training solely on responses, without any corresponding instructions, yields instruction following. This suggests pretrained models have an instruction-response mapping which is revealed by teaching the model the desired distribution of responses. However, we then find it's not necessary to teach the desired distribution of responses: instruction-response training on narrow-domain data like poetry still leads to broad instruction-following behavior like recipe generation. In particular, when instructions are very different from those in the narrow finetuning domain, models' responses do not adhere to the style of the finetuning domain. To begin to explain implicit instruction tuning, we hypothesize that very simple changes to a language model's distribution yield instruction following. We support this by hand-writing a rule-based language model which yields instruction following in a product-of-experts with a pretrained model. The rules are to slowly increase the probability of ending the sequence, penalize repetition, and uniformly change 15 words' probabilities. In summary, adaptations made without being designed to yield instruction following can do so implicitly.",
      "authors": [
        "John Hewitt",
        "Nelson F. Liu",
        "Percy Liang",
        "Christopher D. Manning"
      ],
      "published": "2024-09-21T22:36:22+00:00",
      "updated": "2024-09-21T22:36:22+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14254v1",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2409.14254v1.pdf"
    },
    {
      "arxiv_id": "2508.15239v2",
      "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai",
      "abstract": "Large language models excel at instruction-following in English, but their performance in low-resource languages like Thai remains underexplored. Existing benchmarks often rely on translations, missing cultural and domain-specific nuances needed for real-world use. We present WangchanThaiInstruct, a human-authored Thai dataset for evaluation and instruction tuning, covering four professional domains and seven task types. Created through a multi-stage quality control process with annotators, domain experts, and AI researchers, WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing performance gaps on culturally and professionally specific tasks, and (2) an instruction tuning study with ablations isolating the effect of native supervision. Models fine-tuned on WangchanThaiInstruct outperform those using translated data in both in-domain and out-of-domain benchmarks. These findings underscore the need for culturally and professionally grounded instruction data to improve LLM alignment in low-resource, linguistically diverse settings.",
      "authors": [
        "Peerat Limkonchotiwat",
        "Pume Tuchinda",
        "Lalita Lowphansirikul",
        "Surapon Nonesung",
        "Panuthep Tasawong",
        "Alham Fikri Aji",
        "Can Udomcharoenchaikit",
        "Sarana Nutanong"
      ],
      "published": "2025-08-21T04:54:05+00:00",
      "updated": "2025-09-19T10:08:52+00:00",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.15239v2",
      "primary_category": "cs.CL",
      "query_source": "instruction tuning",
      "local_pdf_path": "data/raw/2508.15239v2.pdf"
    },
    {
      "arxiv_id": "2401.14043v3",
      "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey",
      "abstract": "Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 50 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.",
      "authors": [
        "Haochen Li",
        "Jonathan Leung",
        "Zhiqi Shen"
      ],
      "published": "2024-01-25T09:47:55+00:00",
      "updated": "2024-09-17T04:56:03+00:00",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14043v3",
      "primary_category": "cs.CL",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2401.14043v3.pdf"
    },
    {
      "arxiv_id": "2502.06039v1",
      "title": "Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models",
      "abstract": "Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a \"prompt agent\" that demonstrates how the most effective techniques can be applied in real-world development workflows.",
      "authors": [
        "Marc Bruni",
        "Fabio Gabrielli",
        "Mohammad Ghafari",
        "Martin Kropp"
      ],
      "published": "2025-02-09T21:23:07+00:00",
      "updated": "2025-02-09T21:23:07+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06039v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2502.06039v1.pdf"
    },
    {
      "arxiv_id": "2507.03405v1",
      "title": "Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering",
      "abstract": "The rapid emergence of generative AI models like Large Language Models (LLMs) has demonstrated its utility across various activities, including within Requirements Engineering (RE). Ensuring the quality and accuracy of LLM-generated output is critical, with prompt engineering serving as a key technique to guide model responses. However, existing literature provides limited guidance on how prompt engineering can be leveraged, specifically for RE activities. The objective of this study is to explore the applicability of existing prompt engineering guidelines for the effective usage of LLMs within RE. To achieve this goal, we began by conducting a systematic review of primary literature to compile a non-exhaustive list of prompt engineering guidelines. Then, we conducted interviews with RE experts to present the extracted guidelines and gain insights on the advantages and limitations of their application within RE. Our literature review indicates a shortage of prompt engineering guidelines for domain-specific activities, specifically for RE. Our proposed mapping contributes to addressing this shortage. We conclude our study by identifying an important future line of research within this field.",
      "authors": [
        "Krishna Ronanki",
        "Simon Arvidsson",
        "Johan Axell"
      ],
      "published": "2025-07-04T09:13:50+00:00",
      "updated": "2025-07-04T09:13:50+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.03405v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2507.03405v1.pdf"
    },
    {
      "arxiv_id": "2503.02400v2",
      "title": "Promptware Engineering: Software Engineering for Prompt-Enabled Systems",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into software applications, giving rise to a broad class of prompt-enabled systems, in which prompts serve as the primary 'programming' interface for guiding system behavior. Building on this trend, a new software paradigm, promptware, has emerged, which treats natural language prompts as first-class software artifacts for interacting with LLMs. Unlike traditional software, which relies on formal programming languages and deterministic runtime environments, promptware is based on ambiguous, unstructured, and context-dependent natural language and operates on LLMs as runtime environments, which are probabilistic and non-deterministic. These fundamental differences introduce unique challenges in prompt development. In practice, prompt development remains largely ad hoc and relies heavily on time-consuming trial-and-error, a challenge we term the promptware crisis. To address this, we propose promptware engineering, a new methodology that adapts established Software Engineering (SE) principles to prompt development. Drawing on decades of success in traditional SE, we envision a systematic framework encompassing prompt requirements engineering, design, implementation, testing, debugging, evolution, deployment, and monitoring. Our framework re-contextualizes emerging prompt-related challenges within the SE lifecycle, providing principled guidance beyond ad-hoc practices. Without the SE discipline, prompt development is likely to remain mired in trial-and-error. This paper outlines a comprehensive roadmap for promptware engineering, identifying key research directions and offering actionable insights to advance the development of prompt-enabled systems.",
      "authors": [
        "Zhenpeng Chen",
        "Chong Wang",
        "Weisong Sun",
        "Xuanzhe Liu",
        "Jie M. Zhang",
        "Yang Liu"
      ],
      "published": "2025-03-04T08:43:16+00:00",
      "updated": "2026-01-27T09:40:22+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02400v2",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2503.02400v2.pdf"
    },
    {
      "arxiv_id": "2406.04710v2",
      "title": "Morescient GAI for Software Engineering (Extended Version)",
      "abstract": "The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness - they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of \"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision and roadmap for how such \"Morescient\" GAI models can be engineered, evolved and disseminated according to the principles of open science.",
      "authors": [
        "Marcus Kessel",
        "Colin Atkinson"
      ],
      "published": "2024-06-07T07:38:33+00:00",
      "updated": "2024-12-03T09:51:23+00:00",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04710v2",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2406.04710v2.pdf"
    },
    {
      "arxiv_id": "2509.17096v1",
      "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering",
      "abstract": "Large Language Models are transforming software engineering, yet prompt management in practice remains ad hoc, hindering reliability, reuse, and integration into industrial workflows. We present Prompt-with-Me, a practical solution for structured prompt management embedded directly in the development environment. The system automatically classifies prompts using a four-dimensional taxonomy encompassing intent, author role, software development lifecycle stage, and prompt type. To enhance prompt reuse and quality, Prompt-with-Me suggests language refinements, masks sensitive information, and extracts reusable templates from a developer's prompt library. Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can accurately classify software engineering prompts. Furthermore, our user study with 11 participants shows strong developer acceptance, with high usability (Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in prompt quality and efficiency through reduced repetitive effort. Lastly, we offer actionable insights for building the next generation of prompt management and maintenance tools for software engineering workflows.",
      "authors": [
        "Ziyou Li",
        "Agnia Sergeyuk",
        "Maliheh Izadi"
      ],
      "published": "2025-09-21T14:24:37+00:00",
      "updated": "2025-09-21T14:24:37+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.17096v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2509.17096v1.pdf"
    },
    {
      "arxiv_id": "1707.03869v3",
      "title": "Cognitive Biases in Software Engineering: A Systematic Mapping Study",
      "abstract": "One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering (SE) research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state of the art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles, which investigate 37 cognitive biases, published between 1990 and 2016. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.",
      "authors": [
        "Rahul Mohanani",
        "Iflaah Salman",
        "Burak Turhan",
        "Pilar Rodriguez",
        "Paul Ralph"
      ],
      "published": "2017-07-12T19:05:19+00:00",
      "updated": "2018-10-23T16:32:46+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/1707.03869v3",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/1707.03869v3.pdf"
    },
    {
      "arxiv_id": "0306108v1",
      "title": "Web Engineering",
      "abstract": "Web Engineering is the application of systematic, disciplined and quantifiable approaches to development, operation, and maintenance of Web-based applications. It is both a pro-active approach and a growing collection of theoretical and empirical research in Web application development. This paper gives an overview of Web Engineering by addressing the questions: a) why is it needed? b) what is its domain of operation? c) how does it help and what should it do to improve Web application development? and d) how should it be incorporated in education and training? The paper discusses the significant differences that exist between Web applications and conventional software, the taxonomy of Web applications, the progress made so far and the research issues and experience of creating a specialisation at the master's level. The paper reaches a conclusion that Web Engineering at this stage is a moving target since Web technologies are constantly evolving, making new types of applications possible, which in turn may require innovations in how they are built, deployed and maintained.",
      "authors": [
        "Yogesh Deshpande",
        "San Murugesan",
        "Athula Ginige",
        "Steve Hansen",
        "Daniel Schwabe",
        "Martin Gaedke",
        "Bebo White"
      ],
      "published": "2003-06-18T03:13:44+00:00",
      "updated": "2003-06-18T03:13:44+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/cs/0306108v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/0306108v1.pdf"
    },
    {
      "arxiv_id": "2406.04780v1",
      "title": "Software Engineering for Collective Cyber-Physical Ecosystems",
      "abstract": "Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focusses on treating these systems as \"composites\" (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as \"collectives\" (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this \"collective computing paradigm\" in software engineering, discusses its peculiar challenges, and outlines a path for future research, touching on aspects such as macroprogramming, collective intelligence, self-adaptive middleware, learning, synthesis, and experimentation of collective behaviour.",
      "authors": [
        "Roberto Casadei",
        "Gianluca Aguzzi",
        "Giorgio Audrito",
        "Ferruccio Damiani",
        "Danilo Pianini",
        "Giordano Scarso",
        "Gianluca Torta",
        "Mirko Viroli"
      ],
      "published": "2024-06-07T09:28:22+00:00",
      "updated": "2024-06-07T09:28:22+00:00",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DC",
        "cs.MA",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04780v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2406.04780v1.pdf"
    },
    {
      "arxiv_id": "2105.13961v3",
      "title": "A Study about the Knowledge and Use of Requirements Engineering Standards in Industry",
      "abstract": "Context: The use of standards is considered a vital part of any engineering discipline. So one could expect that standards play an important role in Requirements Engineering (RE) as well. However, little is known about the actual knowledge and use of RE-related standards in industry. Objective: In this article, we investigate to which extent standards and related artifacts such as templates or guidelines are known and used by RE practitioners. Method: To this end, we have conducted a questionnaire-based online survey. We could analyze the replies from 90 RE practitioners using a combination of closed and open-text questions. Results: Our results indicate that the knowledge and use of standards and related artifacts in RE is less widespread than one might expect from an engineering perspective. For example, about 47% of the respondents working as requirements engineers or business analysts do not know the core standard in RE, ISO/IEC/IEEE 29148. Participants in our study mostly use standards by personal decision rather than being imposed by their respective company, customer, or regulator. Beyond insufficient knowledge, we also found cultural and organizational factors impeding the widespread adoption of standards in RE. Conclusions: Overall, our results provide empirically informed insights into the actual use of standards and related artifacts in RE practice and - indirectly - about the value that the current standards create for RE practitioners.",
      "authors": [
        "Xavier Franch",
        "Martin Glinz",
        "Daniel Mendez",
        "Norbert Seyff"
      ],
      "published": "2021-05-28T16:31:25+00:00",
      "updated": "2021-09-06T04:59:14+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.13961v3",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2105.13961v3.pdf"
    },
    {
      "arxiv_id": "1904.04104v1",
      "title": "Software Engineering in Civic Tech: A Case Study about Code for Ireland",
      "abstract": "Civic grassroots have proven their ability to create useful and scalable software that addresses pressing social needs. Although software engineering plays a fundamental role in the process of creating civic technology, academic literature that analyses the software development processes of civic tech grassroots is scarce. This paper aims to advance the understanding of how civic grassroots tackle the different activities in their software development processes. In this study, we followed the formation of two projects in a civic tech group (Code for Ireland) seeking to understand how their development processes evolved over time, and how the group carried out their work in creating new technology. Our preliminary findings show that such groups are capable of setting up systematic software engineering processes that address software specification, development, validation, and evolution. While they were able to deliver software according to self-specified quality standards, the group has challenges in requirements specification, stakeholder engagement, and reorienting from development to product delivery. Software engineering methods and tools can effectively support the future of civic technologies and potentially improve their management, quality, and durability.",
      "authors": [
        "Antti Knutas",
        "Victoria Palacin",
        "Giovanni Maccani",
        "Markus Helfert"
      ],
      "published": "2019-04-08T14:53:37+00:00",
      "updated": "2019-04-08T14:53:37+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/1904.04104v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/1904.04104v1.pdf"
    },
    {
      "arxiv_id": "2510.22251v1",
      "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion",
      "abstract": "Prompt engineering, particularly Chain-of-Thought (CoT) prompting, significantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a constrained, rule-based prompting method designed to improve upon standard CoT by reducing errors from semantic ambiguity and flawed common sense.\n  We evaluate three prompting strategies (Zero Shot, standard CoT, and Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5) using the GSM8K mathematical reasoning benchmark (1,317 problems).\n  Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00% vs. 96.36% for CoT on full benchmark). We trace this to a \"Guardrail-to-Handcuff\" transition where constraints preventing common-sense errors in mid-tier models induce hyper-literalism in advanced models. Our detailed error analysis demonstrates that optimal prompting strategies must co-evolve with model capabilities, suggesting simpler prompts for more capable models.",
      "authors": [
        "Imran Khan"
      ],
      "published": "2025-10-25T11:04:01+00:00",
      "updated": "2025-10-25T11:04:01+00:00",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.22251v1",
      "primary_category": "cs.CL",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2510.22251v1.pdf"
    },
    {
      "arxiv_id": "2507.14735v1",
      "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling",
      "abstract": "The introduction of large language models (LLMs) has enhanced automation in software engineering tasks, including in Model Driven Engineering (MDE). However, using general-purpose LLMs for domain modeling has its limitations. One approach is to adopt fine-tuned models, but this requires significant computational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can improve the accuracy of the Llama 3.1 model for generating domain models from textual descriptions. We use search-based methods to tune hyperparameters for a specific medical data model, resulting in a notable quality improvement over the baseline LLM. We then test the optimized hyperparameters across ten diverse application domains.\n  While the solutions were not universally applicable, we demonstrate that combining hyperparameter tuning with prompt engineering can enhance results across nearly all examined domain models.",
      "authors": [
        "Vladyslav Bulhakov",
        "Giordano d'Aloisio",
        "Claudio Di Sipio",
        "Antinisca Di Marco",
        "Davide Di Ruscio"
      ],
      "published": "2025-07-19T19:49:58+00:00",
      "updated": "2025-07-19T19:49:58+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.14735v1",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2507.14735v1.pdf"
    },
    {
      "arxiv_id": "2011.05106v2",
      "title": "How do Practitioners Perceive the Relevance of Requirements Engineering Research?",
      "abstract": "The relevance of Requirements Engineering (RE) research to practitioners is vital for a long-term dissemination of research results to everyday practice. Some authors have speculated about a mismatch between research and practice in the RE discipline. However, there is not much evidence to support or refute this perception. This paper presents the results of a study aimed at gathering evidence from practitioners about their perception of the relevance of RE research and at understanding the factors that influence that perception. We conducted a questionnaire-based survey of industry practitioners with expertise in RE. The participants rated the perceived relevance of 435 scientific papers presented at five top RE-related conferences. The 153 participants provided a total of 2,164 ratings. The practitioners rated RE research as essential or worthwhile in a majority of cases. However, the percentage of non-positive ratings is still higher than we would like. Among the factors that affect the perception of relevance are the research's links to industry, the research method used, and respondents' roles. The reasons for positive perceptions were primarily related to the relevance of the problem and the soundness of the solution, while the causes for negative perceptions were more varied. The respondents also provided suggestions for future research, including topics researchers have studied for decades, like elicitation or requirement quality criteria.",
      "authors": [
        "Xavier Franch",
        "Daniel Mendez",
        "Andreas Vogelsang",
        "Rogardt Heldal",
        "Eric Knauss",
        "Marc Oriol",
        "Guilherme H. Travassos",
        "Jeffrey C. Carver",
        "Thomas Zimmermann"
      ],
      "published": "2020-11-10T14:09:26+00:00",
      "updated": "2020-12-03T06:56:26+00:00",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2011.05106v2",
      "primary_category": "cs.SE",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2011.05106v2.pdf"
    },
    {
      "arxiv_id": "2002.11672v3",
      "title": "Towards Digital Engineering -- The Advent of Digital Systems Engineering",
      "abstract": "Digital Engineering, the digital transformation of engineering to leverage digital technologies, is coming globally. This paper explores digital systems engineering, which aims at developing theory, methods, models, and tools to support the emerging digital engineering. A critical task is to digitalize engineering artifacts, thus enabling information sharing across platform, across life cycle, and across domains. We identify significant challenges and enabling digital technologies; analyze the transition from traditional engineering to digital engineering; define core concepts, including \"digitalization\", \"unique identification\", \"digitalized artifacts\", \"digital augmentation\", and others; present a big picture of digital systems engineering in four levels: vision, strategy, action, and foundation; briefly discuss each of main areas of research issues. Digitalization enables fast infusing and leveraging novel digital technologies; unique identification enables information traceability and accountability in engineering lifecycle; provenance enables tracing dependency relations among engineering artifacts; supporting model reproducibility and replicability; helping with trustworthiness evaluation of digital engineering artifacts.",
      "authors": [
        "Jingwei Huang",
        "Adrian Gheorghe",
        "Holly Handley",
        "Pilar Pazos",
        "Ariel Pinto",
        "Samuel Kovacic",
        "Andy Collins",
        "Charles Keating",
        "Andres Sousa-Poza",
        "Ghaith Rabadi",
        "Resit Unal",
        "Teddy Cotter",
        "Rafael Landaeta",
        "Charles Daniels"
      ],
      "published": "2020-02-21T04:58:20+00:00",
      "updated": "2020-08-30T22:27:51+00:00",
      "categories": [
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.11672v3",
      "primary_category": "cs.CY",
      "query_source": "prompt engineering techniques",
      "local_pdf_path": "data/raw/2002.11672v3.pdf"
    }
  ]
}